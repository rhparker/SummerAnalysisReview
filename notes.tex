\documentclass[12pt]{amsart}         %% What type of document you're writing.

%%%%% Preamble

%% Packages to use

\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm}   %% AMS mathematics macros
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage[margin=1.0in]{geometry}
\usepackage{float}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
%% Title Information.

\newcommand{\A}{\textbf{A}}
\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\z}{\textbf{z}}
\newcommand{\bb}{\textbf{b}}
\newcommand{\B}{\textbf{B}}
\newcommand{\F}{\textbf{F}} 
\newcommand{\N}{\mathbb{N}} 
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\St}{\mathcal{S}}
\newcommand{\Rt}{\mathcal{R}}
\newcommand{\dst}{\displaystyle}
\newcommand{\set}[2]{\{ #1 | #2 \}}
\newcommand{\hint}[1]{(\textit{Hint:} #1)}
\newcommand{\st}{\text{ such that }}

\title{APMA Summer Program Notes}
\author{Ross Parker}

\begin{document}

\maketitle

\section*{Outline}
\label{sec: introduction}
These are the notes for the 2019 summer workshop for incoming graduate students in applied mathematics at Brown University. These are based on the notes for the 2017 and 2018 workshops, which were led by Bj\"orn Sandstede. The original notes were typed by Rebecca Santorella and Patrick Liscio. Topics covered include:

\begin{itemize}
    \item Metric spaces and compactness
    \item Compactness in $C([a,b])$ (Arzela-Ascoli theorem)
    \item Differentiation in Banach spaces
    \item Banach's fixed-point theorem and the implicit function theorem
    \item Introduction to measure Theory and Lebesgue integration
\end{itemize}

\section{Metric Spaces}
\label{sec: metric_spaces}

\subsection{Definitions}

A metric space is a nonempty set, together with a function called a metric, which defines a distance between any pair of points in the set. One familiar metric space is $\R^2$, the set of points in the Cartesian plane, with the metric given by the Euclidean distance formula. The distance between points $p = (x_1, y_1)$ and $q = (x_2, y_2)$ is given by 
\[
d(p, q) = \sqrt{ (x_2 - x_1)^2 + (y_2 - y_1)^2 },
\]
and represents the ``straight-line distance'' between $p$ and $q$. An arbitrary metric space is defined as follows:

\begin{definition}
Let $X$ be an arbitrary set. A function $d : X \times X \rightarrow \mathbb{R}$ is a \textbf{metric} on $X$ if the following conditions hold for all $x, y, z \in X$:
\begin{enumerate}[(i)]
\item $d(x,y) \geq 0$.
\item $d(x,y) = 0$ if and only if $x = y$.
\item $d(x,y) = d(y,x)$ (symmetry).
\item $d(x,y) \leq d(x,z) + d(z,y)$ (triangle inequality).
\end{enumerate}
We call the pair $(X, d)$ a \textbf{metric space}.
\end{definition}

Property (iv) is called the triangle inequality, since it generalizes the familiar statement from geometry: ``The length of a side of a triangle must be less than or equal to the sum of the lengths of the other two sides''. (If equality holds, the ``triangle'' has been squished down to a line segment, so it's not really a triangle anymore.) From the triangle inequality, we can derive the so-called ``reverse triangle inequality'',
\[
d(x,y) \geq  |d(x,z) - d(z, y)|
\]
which generalizes the statement from geometry: ``The length of a side of a triangle must be greater than or equal to the difference of the lengths of the other two sides''. From now on, we will assume we are working in a metric space $(X, d)$.

\subsection{Examples}

Here are some examples of metrics and metric spaces.

\begin{enumerate}
\item The \emph{Euclidean distance} on $\R^n$ is defined by
\[
d(x,y) = |x-y|.
\]
If $x = (x_1, \dots x_n)$ and $y = (y_1, \dots, y_n)$, then this is given by
\[
d(x,y) = \sqrt{ \sum_{k=1}^n (x_k - y_k)^2 }.
\]
\item The \emph{discrete metric} on any set $X$ is defined by
\[
d(x,y) = 
\begin{cases} 
    0 & \text{ if } x=y \\
    1 & \text{ otherwise }.
\end{cases}
\]
\item Compare three metrics on $\R^2$. For $x = (x_1, x_2)$ and $y = (y_1, y_2)$, we define
\begin{enumerate}
    \item Manhattan distance / taxicab metric / $\ell^1$ metric:
    \[
    d_1(x,y) = |x_1 - y_1| + |x_2 - y_2|.
    \]
    \item Euclidean distance / $\ell^2$ metric:
    \[
    d_2(x,y) = \sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 }.
    \]
    \item Maximum distance / chessboard metric / $\ell^\infty$ metric:
    \[
    d_\infty(x,y) = \max\{ |x_1 - y_1|,  |x_2 - y_2| \}.
    \]
\end{enumerate}

\item The maximum (supremum) metric on $C([a, b])$, the space of continuous, real-valued functions on the closed interval $[a, b]$, is defined by
\[
d(f, g) = \max_{x \in [a,b]}|f(x) - g(x)|.
\]
Since a continuous function attains its maximum and minimum on a closed interval (extreme value theorem), this is well-defined.

\item Information theory:
\begin{enumerate}
    \item Distance between two strings of identical length (Hamming distance): number of positions at which the two strings are different.
    \item Distance between two arbitrary strings (Levenshtein distance): minimum number of single-character edits (insertions, deletions or substitutions) required to change one string into the other.
    \item Damerau-Levenshtein distance: same as  Levenshtein distance, except swaps of adjacent characters are also allowed operations. ``More than 80\% of all human misspellings can be expressed by a single error of one of the four types'' (Damerau, 1964).
\end{enumerate}
    
\item Geodesic distance for connected graphs: number of edges in a shortest path connecting two vertices. (There may not be a unique shortest path, but the geodesic distance is unique). The graph must be connected to avoid having infinite distances.

\end{enumerate}

\subsection{Product metrics}

Recall that the (Cartesian) product of two sets $X$ and $Y$, denoted $X \times Y$, is the set of all ordered pairs $(x,y)$, where $x \in X$ and $y \in Y$. The product of a finite sequence of sets $X_1, \dots, X_n$ is the set of ordered $n$-tuples
\[
\prod_{k=1}^n X_k = X_1 \times \dots \times X_n := \{ (x_1, \dots, x_n)  : x_k \in X_k \}.
\]
Take a finite sequence of metric spaces $(X_1, d_1), \dots, (X_n, d_n)$. Note that each set can have a different metric if we like. Then any of the following are metrics on $X_1 \times \dots \times X_n$. We call these product metrics.
\begin{enumerate}
    \item $\begin{aligned}d(x, y) = \sum_{k=1}^n d(x_k, y_k) \end{aligned}$.
    \item $\begin{aligned}d(x, y) = \sqrt{ \sum_{k=1}^n d(x_k, y_k)^2  } \end{aligned}$.
    \item $\begin{aligned}d(x,y) \max_{k = 1, \dots, n} d(x_k, y_k) \end{aligned}$.
\end{enumerate}

We can also construct metrics on \emph{countable} products of metric spaces. The countable product of sets $\{X_k\}$ is the set of all sequences
\[
\prod_{k=1}^\infty X_k := \Big\{ \{x_k\} : x_k \in X_k \Big\}.
\]
In order to define a metric on this countable product, we use the following trick, which allows us to construct metrics from other metrics. If $f:[0, \infty) \rightarrow [0, \infty)$ is an increasing concave function such that $f(x)=0$ if and only if $x=0$, then $f(d(x,y))$ is also a metric. Note that $f$ does not have to be smooth. Important examples are:
\begin{enumerate}
    \item $\begin{aligned}f(x) = \min\{x, 1\}\end{aligned}$.
    \item $\begin{aligned}f(x) = \frac{x}{1+x}\end{aligned}$.
\end{enumerate}
Both of these functions ``cut off'' the metric at 1, so that the greatest possible distance between any two points is 1. We can use the second of these ``cutoff'' functions to define a metric for a countable product of metric spaces $\{(X_k, d_k)\}$:
\[
d(x,y) = \sum_{k = 1}^\infty \frac{1}{2^k} \frac{d_k(x_k, y_k)}{1+d_k(x_k, y_k)}.
\]
Alternatively, we could use the ``cutoff'' metric $\min\{d_k(x_k, y_k), 1\}$. We can also replace $1/2^k$ with the terms from any convergent series of positive terms.

\subsection{Sequences}

First, we define the convergence of a sequence in a metric space. Intuitively, a sequence $\{x_n\}$ converges to a limit $L$ if the elements of the sequence get arbitrarily close to $L$.
\begin{definition}
A sequence $\{x_n\}$ \emph{converges} to $L$, written $x_n \rightarrow L$, if $d(x_n, L) \rightarrow 0$.
\end{definition}
The condition $d(x_n, L) \rightarrow 0$ is the convergence of a real-valued sequence, which is defined in the standard way. We can also write this as follows: $x_n \rightarrow L$ if, for all $\epsilon > 0$, there exists $N \in \N$ such that $d(x_n, L) < \epsilon$ for all $n \geq N$. 

Another important type of sequence in a metric space is a Cauchy sequence. Intuitively, a sequence $\{x_n\}$ is a Cauchy sequence if its elements get arbitrarily close to \emph{each other} (rather than approach a limit).
\begin{definition}
$\{x_n\}$ is a \textbf{Cauchy sequence} if for all $\epsilon > 0$, there exists $N \in \N$ such that $d(x_n, x_m) < \epsilon$ for all $n,m \geq N$.
\end{definition}
Since this is annoying to write, we can notate this as $d(x_m, x_n) \rightarrow 0$. Every convergent sequence is a Cauchy sequence (this follows from the triangle inequality). The converse is unfortunately not true in general. That being said, there are metric spaces in which every Cauchy sequence is convergent (even if we cannot determine what that limit is!) Since this property is so important, we have the following definition.

\begin{definition}A metric space is \textbf{complete} if every Cauchy sequence is convergent.
\end{definition}

An example of a metric space which is not complete is $\mathbb{Q}$ with $d(x,y) = |x-y|$. To see this, take the sequence $\{x_n\}$, with $x_1 = 1$ and 
\[
x_{n+1} = \frac{x_n}{2} + \frac{1}{x_n}.
\]
This is a Cauchy sequence, but its limit is $\sqrt{2}$, which is not in $\Q$.

An example of a complete metric space is $\R$ (the completeness of $\R$ follows from its construction). Using the product metric on $\R^n$ (the maximum version of the metric is easiest here) and the completeness of $\R$, it follows that $\R^n$ is complete. Since $\C$ is isomorphic to $\R^2$, $\C^n$ is complete as well.

At this point, we have two ways to show a sequence $\{x_n\}$ converges.
\begin{enumerate}
    \item Use the definition of convergence to show that $x_n \rightarrow L$. This means that we need a guess for what $L$.
    \item Work in a complete metric space, and show $\{x_n\}$ is a Cauchy sequence. This is often easier, since we do not need a guess for the limit, but it has the drawback of not giving us the actual limit.
\end{enumerate}

\subsection{Limits and Continuity}

First we define the limit of a function between two metric spaces. 

\begin{definition}Let $f: (X, d_1) \rightarrow (Y, d_2)$. Then $f(x) \rightarrow L$ as $x \rightarrow x_0$ if either of the following equivalent definitions holds:
\begin{enumerate}
\item For every $\epsilon > 0$ there exists $\delta > 0$ (dependent on $\epsilon$ and $x_0$) such that if $d_2(x, x_0) < \delta$, $d_2(f(x), L) < \epsilon$.
\item For every sequence $\{x_n\}$ with $x_n \rightarrow x_0$, $f(x_n) \rightarrow L$.
\end{enumerate}
\end{definition}

We can use this to define continuity of a function (we will call this the metric space definition of continuity).

\begin{definition}
$f : (X, d_1) \rightarrow (Y, d_2)$ is \textbf{continuous at $x_0$} if $f(x) \rightarrow f(x_0)$ as $x \rightarrow x_0$. $f$ is \textbf{continuous} if $f$ is continuous at $x_0$ for all $x_0 \in X$.
\end{definition}

We discussed above that $C([a,b])$ is a metric space with the maximum (supremum) metric. Our goal is to show that $C([a,b])$ is a complete metric space. To do that, first we show that the metric itself is a continuous function.

\begin{proposition}
The metric $d$ is a continuous function from $X \times X \rightarrow \R$.
\begin{proof}
Let $D\left((a,b),(x,y)\right) = d(a,x)+d(b,y)$ be the product metric on $X \times X$. Let $(x_n, y_n) \rightarrow (x, y)$ in $(X \times X, D)$. We will show that $d(x_n,y_n) \rightarrow d(x,y)$, which is equivalent to showing that $|d(x_n, y_n) - d(x,y)| \rightarrow 0$. Using the triangle inequality on $\R$ and the reverse triangle inequality on $X$,
\begin{align*}
    |d(x_n, y_n) - d(x,y)| &\leq |d(x_n, y_n) - d(x, y_n)| + |d(x, y_n) - d(x,y)| \\
    &= d(x_n, x) + d(y_n, y) \\
    &= D((x_n, y_n), (x, y)) \\
    &\rightarrow 0.
\end{align*}
\end{proof}
\end{proposition}

Next, we define the following two modes of convergence. (There are others, which you will study in courses in real analysis and probability). For simplicity, we will only consider real-valued functions here, but the codomain can be any metric space (with the appropriate adjustments).

\begin{definition}
Consider a sequence of functions $\{f_n\}$, $f_n:(X, d) \rightarrow \R$. 
\begin{enumerate}
    \item $f_n \rightarrow f$ \textbf{pointwise} if $|f_n(x) - f(x)| \rightarrow 0$ for all $x \in X$. In other words, if you give me $x$ and $\epsilon > 0$, I can find a natural number $N = N(x, \epsilon)$ such that $|f_n(x) - f(x)| < \epsilon$ whenever $n \geq N$.
    \item $f_n \rightarrow f$ \textbf{uniformly} if the rate of convergence does not depend on $x$. In other words, if you give me $\epsilon > 0$, I can find a natural number $N = N(\epsilon)$ (independent of $x$) such that for all $x \in X$, $|f_n(x) - f(x)| < \epsilon$ whenever $n \geq N$. This is equivalent to
    \[
    \sup_{x \in X}|f_n(x) - f(x)| \rightarrow 0.
    \]
\end{enumerate}
\end{definition}

The supremum metric (also called the uniform metric) on the space of real-valued functions on $(X, d)$ is defined by
\[
D(f,g) = \sup_{x \in X}|f(x) - g(x)|.
\]
Uniform convergence is convergence with respect to this metric. Next, we prove the uniform limit theorem, which says that the uniform limit of continuous functions is continuous.

\begin{theorem}[Uniform Limit Theorem]
Let $\{f_n\}$ be a sequence of continuous functions on a metric space $(X,d)$.
If $f_n \rightarrow f$ uniformly, then $f$ is continuous as well.
\begin{proof}
We will show that $f$ is continuous at $x_0 \in X$. Choose any $\epsilon > 0$.
\begin{enumerate}
\item Since $f_n \rightarrow f$ uniformly, we can find $N \in \N$ such that $\sup_{x \in X}|f_n(x) - f(x)| < \epsilon$ for all $n \geq N$.
\item Since $f_N$ is continuous at $x_0$, we can find $\delta > 0$ such that if $d(x, x_0) < \delta$, $|f_N(x) - f_n(x_0)| < \epsilon$. 
\item By the triangle inequality, as long as $d(x, x_0) < \delta$,
\[
|f(x) - f(x_0)| \leq |f(x) - f_N(x)| + |f_N(x) - f_N(x_0)| + |f_N(x_0) - f(X_0)| \leq 3 \epsilon
\]
\end{enumerate}
\end{proof}
\end{theorem}

Using all of this, we can show that $C([a,b])$ is a complete metric space, i.e. every Cauchy sequence is convergent.

\begin{theorem}
The space $C([a,b])$ with the uniform metric is a complete metric space.
\begin{proof}
Let $\{ f_n(x) \}$ be a Cauchy sequence in $C([a,b])$. Our goal is to find a function $f \in C([a,b])$ such that $f_n \rightarrow f$, i.e $\sup_{x \in X}|f_n(x) - f(x)| \rightarrow 0$.
\begin{enumerate}
    \item Since for all $x \in [a,b]$, $| f_n(x) - f_m(x)| \leq \sup| f_n(x) - f_m(x)| \rightarrow 0$, $\{ f_n(x) \}$ is a Cauchy sequence for each $x \in [a,b]$.
    \item By the completeness of $\R$, for each $x \in [a,b]$, $f_n(x)$ converges to some limit in $\R$. Call this limit $f(x)$.
    \item Next, we show that $f_n \rightarrow f$ uniformly, i.e. $\sup_{x \in X}| f_n(x) - f(x)| \rightarrow 0$. For any $\epsilon > 0$, choose $N \in \N$ such that for all $x \in [a, b]$ and all $m, n \geq N$, $|f_n(x) - f_m(x)| < \epsilon$. Since the metric $d(a,b) = |a - b|$ (i.e. the absolute value on $\R$) is continuous, send $m \rightarrow \infty$ to get $|f_n(x) - f(x)| < \epsilon$. Since this is true for all $x \in [a,b]$, $\sup_{x \in X}|f_n(x) - f(x)| < \epsilon$, i.e. $f_n \rightarrow f$ uniformly on $[a,b]$.
    \item $f(x)$ is continuous by the uniform limit theorem.
\end{enumerate}
\end{proof}
\end{theorem}

\subsection{Open and Closed Sets}

In this section, we define open and closed sets of a metric space $(X,d)$.

\begin{definition}
The \textbf{$\epsilon$-neighborhood} or \textbf{open $\epsilon-$ball} of $x_0$ is the set
\[
B_\epsilon(x) = \{ x \in X : d(x,x_0) < \epsilon \}
\]
You may see this notated as $B(x, \epsilon)$ or $U_\epsilon(x)$.
\end{definition}

We use this to define an open set in a metric space.

\begin{definition}
A subset $U \subset X$ is \textbf{open} if for every $x \in U$ there exists $\epsilon > 0$ such that $B_\epsilon(x) \subset U$.
\end{definition}

In other words, we can find an open ball around any point in $U$ which is entirely contained in $U$. This ``open ball property'' is so useful, that we can give it a name.

\begin{definition}
$x$ is an \textbf{interior point} of $E$ there exists $\epsilon > 0$ such that $B_\epsilon(x) \subset U$.
\end{definition}

We can then say that a set $U$ is open if it consists entirely of interior points. We also define a closed set.

\begin{definition}
A subset $K \subset X$ is \textbf{closed} if whenever $\{x_n\} \subset K$ with $x_n \rightarrow x$, $x \in K$.
\end{definition}

The half-open interval $(0,1]$ is not closed, since the sequence $\left\{ \frac{1}{2^k} \right\}$ converges to $0$, but $0 \notin (0,1]$. Sometimes you will see closed sets defined in terms of limit points.

\begin{definition}
A point $x$ is a \textbf{limit point} of a set $E$ if for every $\epsilon > 0$, $B_\epsilon$ contains a point in $E$ which is not $x$. (Note that $x$ may or may not be in $E$).
\end{definition}

The point $0$ is a limit point of the half-open interval $(0,1]$, but $0 \notin (0,1]$. It follows from this definition that if $x$ is a limit point of $E$, there exists a sequence $\{x_n\} \subset E$ such that $x_n \rightarrow x$. We then have the following alternative definition of a closed set.

\begin{definition}
A subset $E \subset X$ is \textbf{closed} if it contains all of its limit points.
\end{definition}

We can show that that if $K$ is closed, its complement $X\setminus K$ is open. In fact, topologists use this as the definition of a closed set, i.e. they say that $K \subset X$ is closed if $X\setminus K$ is open. We can also show the following properties of open and closed sets.

\begin{enumerate}
    \item Arbitrary unions of open sets are open.
    \item Finite intersections of open sets are open.
    \item Arbitrary intersections of closed sets are closed.
    \item Finite unions of closed sets are closed.
\end{enumerate}

Topologists actually use these properties to define open sets! A topology on a set $X$ is a family $\mathcal{T}$ of subsets of $X$ which contains $X$ and the empty set; is closed under arbitrary unions; and is closed under finite intersections. The sets in $\mathcal{T}$ are called then called the open sets. 

Finally, we define the closure, interior, and boundary of a set. For all of these, several equivalent definitions are given.

\begin{definition}
The \emph{closure} of a set $E$, denoted $\overline{E}$, is defined in any of the following ways:
\begin{enumerate}
    \item All points in $E$ together with all limit points of $E$.
    \item The intersection of all closed sets containing $E$.
    \item The smallest closed set containing $E$ (i.e. if $K$ is a closed set containing $E$, then $K$ contains $\overline{E}$).
\end{enumerate}
\end{definition}

\begin{definition}
The \emph{interior} of a set $E$, denoted $E^\mathrm{o}$, is defined in any of the following ways:
\begin{enumerate}
    \item The set of all interior points of $E$.
    \item The union of all open sets contained in $E$.
    \item The largest open set contained in $E$ (i.e. if $U$ is an open set contained in $E$, then $U \subset E^\mathrm{o}$).
\end{enumerate}
\end{definition}

\begin{definition}
The \emph{boundary} of a set $E$, denoted $\partial E$, is defined in any of the following ways:
\begin{enumerate}
\item The closure of $E$ without the interior of $E$: $\partial E = \overline{E}\setminus E^\mathrm{o}$.
\item The set of points $x$ such that every ball $B_{\epsilon}(x)$ contains at least one point of $E$ and at least one point of $X\setminus E$.
\end{enumerate}

We note that the boundary of a set is always closed.

\end{definition}

\subsection{Equivalent Metrics}

Sometimes it does not matter much what metric we use, i.e. different metrics give us the same convergent sequences and the same open sets. 

\begin{definition}
Two metrics $d_1$ and $d_2$ on $X$ are \textbf{strongly equivalent} if there exist constants $C_1$ and $C_2$ such that for all $x, y \in X$,
\[
C_1 d_1(x,y) \leq d_2(x,y) \leq C_2 d_1(x,y)
\]
\end{definition}

If two metrics on $X$ are strongly equivalent, the open sets and convergent sequences of $X$ are the same; there may, however, be other differences between the spaces. For finite dimensional spaces such as $\R^n$, the Euclidean, taxicab, and maximum metrics are all strongly equivalent. For example, for the maximum and taxicab metrics, we have
\[
\max_{k=1, \dots, n}|x_k - y_k| \leq \sum_{k=1}^n |x_k - y_k| \leq n \max_{k=1, \dots, n}|x_k - y_k|
\]
so the definition of strong equivalence is satisfied with with $C_1 = 1$ and $C_2 = n$. It is a good exercise to draw the unit balls in the taxicab, Euclidean, and maximum metric in $\R^2$. The take-home message is that it the shape of the open balls usually don't matter too much.

\subsection{Topological Definition of Continuity}

It is sometimes easier to use the topological definition of continuity rather than the metric space definition. This way you can avoid all of those pesky $\epsilon$s and $\delta$s! Topologists define continuity as follows.

\begin{definition}
A function $f: X \rightarrow Y$ is \textbf{continuous} if $f^{-1}(U)$ is open in $X$ for each open set $U \subset Y$.
\end{definition}

Fortunately, the metric space definition of continuity is equivalent to the topological definition of continuity. 

\begin{theorem}
$f: (X, d_1) \rightarrow (Y, d_2)$ is continuous $\iff$ $f^{-1}(U)$ is open in $X$ for each open set $U \subset Y$.
\begin{proof}We prove this in both directions.

\indent $(\implies)$ Suppose $f$ is continuous using the metric space definition. Let $U \subset Y$ be an open set. We need to show that $f^{-1}(U)$ is open. Let $x_0 \in f^{-1}(U)$, so that $y_0 = f(x_0) \in U$. Since $U$ is open, $y_0 \in B_\epsilon(y_0) \subset U$. By continuity of $f$, there exists $\delta > 0$ such that $x \in B_\delta(x_0)$ implies $f(x) \in B_\epsilon(y_0)$. Since $B_\epsilon(y_0) \in U$, $x_0 \in B_\delta(x_0) \subset f^{-1}(U)$.\\

\indent $(\impliedby)$ Suppose $f$ is continuous using the topological definition. Choose any $x_0$ and any $\epsilon > 0$, and let $y_0 = f(x_0)$. Let $U = B_\epsilon(y)$, which is open. Then $f^{-1}(U)$ is open and contains $x_0$, thus there exists $\delta > 0$ such that $x_0 \in B_\delta(x_0) \subset f^{-1}(U)$. By definition of $U$, $d(f(x), f(x_0)) < \epsilon$ whenever $d(x, x_0) < \delta$.
\end{proof}
\end{theorem}

Since the complement of a open set is a closed set, and since the inverse image commutes with all set operations, we also have the following equivalent statement: $f: X \rightarrow Y$ is continuous if $f^{-1}(K)$ is closed in $X$ for each closed set $K \subset Y$.

\subsection{Connectedness}

Intuitively, a disconnected set has two (or more!) ``separate pieces''. We state this mathematically as follows. There are many equivalent definition of connectedness. I like this one.

\begin{definition}
A subset $E$ is \textbf{disconnected} if we can find two disjoint, nonempty open sets $A$ and $B$ such that $E \subset A \cup B$ and both $E \cap A$ and $E \cap B$ are nonempty; $E$ is \textbf{connected} if this is not possible.
\end{definition}

Here is another common definition you will see (i.e. the one in Rudin and Munkres). The advantage of this definition is that it defines a disconnected set as actually being split up into two separate pieces. 

\begin{definition}
Two sets $A$ and $B$ are \textbf{separated} if both $A \cap \overline{B}$ and $\overline{A}\cap B$ are empty. A subset $E$ of $X$ is \textbf{disconnected} if it is the union of two nonempty, separated sets. A subset $E$ of $X$ is \textbf{connected} if this is not possible.
\end{definition}

In the case of the real line, we can say exactly what the connected subsets are. Before we do that, we define an interval in the real line.

\begin{definition}
A subset $I \subset \R$ is an \textbf{interval} if, for all $x$ and $y$ in $I$, every real number between $x$ and $y$ is in $I$ as well.
\end{definition}

Examples of intervals include the open intervals $(a,b)$, the closed intervals $[a,b]$, and unbounded intervals such as $[a, \infty)$. We then have the following proposition.

\begin{proposition}
The connected subsets of $\R$ are intervals and points.
\begin{proof}
Let $E$ be a subset of $\R$. As an edge case, if $E$ contains a single point, then $E$ is connected. Therefore, we can assume that $E$ contains at least two points. We will show that $E$ is connected $\iff$ $E$ is an interval, by proving the two contrapositives.

\indent $(\implies)$ Show that if $E$ is not an interval, than $E$ is not connected. Suppose $E$ is not an interval. Then we can find $x < y < z$ with $x, z \in E$ and $y \notin E$. Then $E \subset (-\infty, y)\cup(y,\infty)$, which implies that $E$ is not connected.\\

\indent $(\impliedby)$ Show that if $E$ is not connected, then $E$ is not an interval.
\begin{enumerate}    
    \item Suppose $E$ is not connected. Then we can write $E = A \cup B$, where $A$ and $B$ are separated, i.e. both $A \cap \overline{B}$ and $\overline{A}\cap B$ are empty.
    \item Let $x \in A$, $y \in B$, and without loss of generality, take $x < y$. 
    \item Let $z = \sup \left(A \cap [x, y]\right)$. Then $z \in \overline{A}$. Since $A$ and $B$ are separated, $z \notin B$, which implies $x \leq z < y$. There are two possibilities to consider: $z \notin A$ and $z \in A$.
    \item If $z \notin A$, then $x < z < y$ with $z \notin A\cup B = E$, so $E$ is not an interval.
    \item If $z \in A$, then $z \notin \overline{B}$. Therefore we can find $w \in (z, y)$ such that $w \notin B$. (If this were not possible, then $z$ would be in $\overline{B}$). Since $w > z$, $w \notin A$ as well (by the definition of $z$ as the supremum). Since $x \leq z < w < y$ and $w \notin A\cup B = E$, $E$ is not an interval.
\end{enumerate}
\end{proof}
\end{proposition}

Using this, we can completely characterize the open sets of $\R$.

\begin{proposition}
Any nonempty open set in $\R$ is the finite or countable union of disjoint open intervals.
\begin{proof}
Let $U$ be an open set in $\R$.
\begin{enumerate}
\item On the set $U$, define the equivalence relation $x \sim y$ if $x, y \in I \subset U$, where $I$ is connected. This partitions $U$ into disjoint equivalence classes, which are the called the \textbf{connected components} of $U$.
\item Let $U_x$ be the connected component containing $x$. This is the largest connected subset of $U$ which contains $x$. In other words, if $I$ is an interval containing $x$, then $I \subset U_x$.
\item By the property of equivalence classes, we can write $U$ as the union of disjoint equivalence classes. In this case, that means we can find a subset $F \subset E$ such that 
\[
U = \bigcup_{x \in F}U_x,
\]
and the elements in the union are all disjoint.
\item Next, we show each $U_x$ is an open interval. Since $U_x$ is connected, it is an interval (by the previous proposition). Let $y \in U_x$. Since $U$ is open, we can find $\epsilon > 0$ such that $B_\epsilon(y) = (y-\epsilon, y+\epsilon) \subset U$. We will show that $B_\epsilon(y) \subset U_x$.
\item We note that $B_\epsilon(y)$ and $U_y$ are both intervals containing $y$. Since $U_y$ is the largest interval containing $y$, $B_\epsilon(y) \subset U_y$.
\item Since the element $y$ is shared by the equivalence classes $U_y$ and $U_x$, it follows that $U_x = U_y$. This implies that $B_\epsilon(y) \subset U_x$, therefore $U_x$ is open.
\item We have shown that $U$ is the union of disjoint open intervals. Since the rational numbers $\Q$ are dense in $\R$, we can find a rational number inside each of these intervals; furthermore, these rational numbers are distinct, since the open intervals are disjoint. Since $\Q$ is countable, we conclude that $U$ is the disjoint union of at most countably many open intervals.
\end{enumerate}
\end{proof}
\end{proposition}

Finally, we show that connectedness is a topological property, i.e. it is preserved by continuous functions.

\begin{theorem}
If $E \subset X$ is connected and $f:X\rightarrow Y$ is continuous, then $f(E)$ is connected.
\begin{proof} We employ proof by contradiction. Assume that $E$ is connected.
\begin{enumerate}
    \item Suppose the conclusion is not true, i.e. $f(E)$ is disconnected. Then, by the definition of a disconnected set, $f(E) \subset A \cup B$, where $A$, $B$ are disjoint, nonempty open sets with $F(E) \cap A \neq \emptyset$ and $F(E) \cap B \neq \emptyset$. 
    \item It follows that $E \subset f^{-1}(A) \cup f^{-1}(B)$, both of which are open by the continuity of $f$.
    \item Since $F(E) \cap A \neq \emptyset$, $f^{-1}(A) \neq \emptyset$ and $f^{-1}(A) \cap E \neq \emptyset$. Similarly, $f^{-1}(B) \neq \emptyset$ and $f^{-1}(B) \cap E \neq \emptyset$.
    \item Finally, since the inverse image commute with set operations, $f^{-1}(A) \cap f^{-1}(B) = f^{-1}(A \cap B) = \emptyset$.
    \item Thus $E$ is disconnected, which is not true.
\end{enumerate}
\end{proof}
\end{theorem}

The intermediate value theorem from calculus follows directly from this.

\begin{corollary}[Intermediate Value Theorem (IVT)]Let $f$ be continuous on $[a, b]$. Then for every $c$ (strictly) between $f(a)$ and $f(b)$ there exists $x \in (a, b)$ such that $f(x) = c$.
\end{corollary}

What is curious (and really cool!) is that the derivative of a function also has the intermediate value property, regardless of whether the derivative itself is continuous.

\begin{theorem}[Darboux]
Suppose $f:\R \rightarrow \R$ is differentiable, and let $a < b$. Then for every $c$ (strictly) between $f'(a)$ and $f'(b)$, there exists $x \in [a, b]$ such that $f'(x) = c$.
\begin{proof} We proceed in the following steps.
\begin{enumerate}
    \item Without loss of generality, take $f'(a) < c < f'(b)$ (otherwise replace $f$ with $-f$).
    \item Define $g(x) = f(x) - cx$. Since $g$ is continuous and $[a,b]$ is closed, $g$ must attain a minimum somewhere on $[a,b]$.
    \item First, we show that this minimum cannot be at $a$. If $g$ attains its minimum at $a$, then $g(x) - g(a) \geq 0$ on $[a, b]$, thus for $x in (a, b]$,
    \[
    \frac{g(x) - g(a)}{x-a} \geq 0.
    \]
    This implies from the definition of the derivative that $g'(a) \geq 0$, which cannot be the case since 
    \[
    g'(a) = f'(a) - c < 0.
    \]
    \item Similarly, we show that this minimum cannot be at $b$.
    \item Therefore, the minimum is at a point $x \in (a,b)$. Since this is a local minimum, $g'(x) = 0$, which implies $f'(x) = c$.
\end{enumerate}
\end{proof}
\end{theorem}

\subsection{Compactness}

Subsets of $\R^n$ that are closed and bounded (e.g. closed boxes) have nice properties. The concept of compactness generalizes this idea of ``closed and bounded'' to arbitrary spaces. A rough overview of the development of the mathematical concept of compactness is as follows. The first important result was proved by Bolzano in 1817 (as a lemma to prove the IVT), and then rediscovered by Weierstrass 50 years later. The theorem (which was originally proved on $\R$) bears both of their names.

\begin{theorem}[Bolzano-Weierstrass]Every bounded sequence in $\R^n$ has a convergence subsequence.
\begin{proof}
Bolzano's proof uses the bisection method, a.k.a. ``slicey-dicey''. For convenience, we will prove the theorem on $\R$, although the proof works the same way for $\R^n$. Let $\{ x_n \}$ be a bounded sequence in $\R$. Then $\{ x_n \}$ fits inside a closed interval $I_1 = [a,b]$.
\begin{enumerate}
    \item First, suppose $\{ x_n \}$ has finite range $\{a_1, \dots, a_n \}$, i.e. there are only finitely many distinct terms in the sequence. Then one of these, say $a_k$, must appear infinitely many times. This implies that the constant subsequence consisting only of the element $a_k$ is a convergent subsequence.
    \item With that out of the way, we may assume $\{ x_n \}$ has infinite range. Cut $I_1$ in half. Then (at least) one half must contain infinitely many terms of $\{ x_n \}$. Call that half $I_2$. Cut $I_2$ in half, and let $I_3$ be the half which contains infinitely many terms of $\{ x_n \}$. Keep doing this to get a sequence of closed, nested intervals $I_1 \supset I_2 \supset I_3 \supset \dots$. The lengths of these intervals go to 0, since each interval has half the length of the previous one.
    \item We can show that the intersection of a sequence of closed, nested intervals $\{I_k\}$ is nonempty (this is sometimes called the Nested Intervals Theorem). Since the interval lengths go to 0, this intersection must be exactly one point $x^*$. (The same idea works in $\R^n$ for nested boxes).
    \item It follows that $x^*$ is a limit point of $\{ x_n \}$. Since $x^*$ is a limit point of $\{ x_n \}$, we can find a subsequence $\{x_{n_k}\}$ of $\{ x_n \}$ which converges to $x^*$.
\end{enumerate}
\end{proof}
\end{theorem}

In the late 19th century, a similar idea was applied to spaces of functions, in particular $C([a,b])$. A teaser is the following theorem. We will define equicontinuity in the next section.

\begin{theorem}[Arzela-Ascoli]Every sequence in $C([a,b])$ which is uniformly bounded and equicontinuous has a convergence subsequence.
\end{theorem}

The term compactness was first used by Frechet in 1905 to describe a set for which every sequence has a convergent subsequence. Today, this property is usually called sequential compactness.

\begin{definition}
A subset $K$ of $X$ is \textbf{sequentially compact} if every sequence in $K$ contains subsequence which converges to an element of $K$.
\end{definition}

The topological definition of compactness has its origins with Heine in 1870, who proved that every continuous function on $[a,b]$ is uniformly continuous (we will define that shortly). In the process, he proved that, given a family of countably many open intervals which cover $[a, b]$, you can select a finite number of them which still cover $[a, b]$. This was generalized into the modern, topological definition of compactness.

\begin{definition}
A subset $K$ of $X$ is \textbf{compact} if any open cover of $K$ has a finite subcover. In other words, given any collection of open sets whose union contains $K$, you can select a finite number of them whose union still contains $K$.
\end{definition}

It turns out that, in metric spaces, compactness and sequential compactness are equivalent. In fact, as we shall soon see, there are three equivalent criteria. Before we get to that, we need one more definition.

\begin{definition}
A subset $K$ of $X$ is \textbf{totally bounded} if, for every $\epsilon > 0$, we can find a finite set of elements $\{ x_1, \dots, x_n \} \subset K$ such that 
\[
K \subset \bigcup_{k=1}^n B_\epsilon(x_k).
\]
In other words, for any $\epsilon > 0$, $K$ can be covered by a finite number of $\epsilon$-balls.
\end{definition}

We start with the following proposition, which is an interesting result in its own right.

\begin{proposition}
Suppose $\{x_n\}$ is a Cauchy sequence, and contains a subsequence $\{x_{n_k} \}$ which converges to $x^*$. Then $x_n \rightarrow x^*$.
\begin{proof}
By the triangle inequality,
\[
d(x_n, x^*) \leq d(x_n, x_{n_k}) + d(x_{n_k}, x^*) \rightarrow 0
\]
as $n, k \rightarrow \infty$.
\end{proof}
\end{proposition}

We can now state the main equivalence theorem regarding compactness in metric spaces.

\begin{theorem}[Compact Equivalence Theorem]
Let $(X,d)$ be a metric space with $K \subset X$. Then the following are equivalent:
\begin{enumerate}[(i)]
\item $K$ is compact.
\item $K$ is sequentially compact.
\item $K$ is complete and totally bounded.
\end{enumerate}

\begin{proof} We will prove that (i)$\implies$(ii), (ii)$\implies$(iii), and (iii)$\implies$(i).

(i)$\implies$(ii): Suppose $K$ is compact. We employ proof by contradiction. Let $\{x_n\}$ be a sequence in $K$, and assume (for a contradiction) that $\{x_n\}$ does not have a convergent subsequence, i.e. no subsequence converges to any element of $K$. Then for every $x \in K$, we can find a radius $\epsilon(x)$ such that $B_{\epsilon(x)}(x)$ contains only finitely many elements of $\{x_n\}$. Since the set of all open balls $B_{\epsilon(x)}(x)$ is an open cover for $K$, and $K$ is compact, we can find a finite subcover for $K$. In other words, we can find a finite set of points $x_1, \dots, x_n \in K$ such that 
\[
K \subset \bigcup_{k=1}^n B_{\epsilon(x_k)}(x_k).
\]
Since each $B_{\epsilon(x)}(x)$ contains finitely many elements of the sequence $\{x_n\}$, this implies that $K$ only contains finitely many elements of the sequence $\{x_n\}$, which is impossible.\\

(ii)$\implies$(iii): Suppose that $K$ is sequentially compact.
\begin{enumerate}
\item First, we show that $K$ is complete, i.e. all Cauchy sequences converge. Let $\{x_n\}$ be a Cauchy sequence in $K$. By sequential compactness, $\{x_n\}$ has a subsequence which converges to $x^* \in K$. By the prior proposition, $x_n \rightarrow x^*$, and so $K$ is complete.
\item We employ proof by contradiction. Suppose $K$ is not totally bounded. Then there exists $\epsilon > 0$ such that $K$ cannot be covered by finitely many open balls $B_\epsilon(x_k)$. Define a sequence $\{x_n\}$ as follows. Start by choosing any $x_1 \in K$. Then choose
\begin{align*}
x_2 &\in K\setminus B_\epsilon(x_1) \\
x_3 &\in K\setminus \left(B_\epsilon(x_1) \cup B_\epsilon(x_2) \right) \\
x_3 &\in K\setminus \left(B_\epsilon(x_1) \cup B_\epsilon(x_2)) \cup B_\epsilon(x_3)\right) \\
&\vdots
\end{align*}
In other words, each element $x_k$ in the sequence lies outside all of the previous $\epsilon$-balls. This process never terminates, otherwise $K$ could in fact be covered by finitely many $\epsilon$-balls. By sequential compactness, $\{x_n\}$ has a convergent subsequence, but this is impossible since $d(x_j, x_k) \geq \epsilon$ for all $j \neq k$.
\end{enumerate}

(iii)$\implies$(i): Suppose $K$ is complete and totally bounded. Once again, we employ proof by contradiction. Let $\{U_\alpha\}_{\alpha \in \mathcal{A}}$ be an open cover of $K$, and assume that there is no finite subcover. We construct the following sequence of sets.
\begin{enumerate}
\item Take $\epsilon_1 = 1/2$. Since $K$ is totally bounded, we can find points $y_1^1, \dots, y_{n(1)}^1$ such that
\[
K \subset \bigcup_{k=1}^{n(1)} B_{1/2}(y_k^1).
\]
Since no finite subcover of $\{U_\alpha\}$ covers $K$, no finite subcover can cover at least one of the open balls $B_{1/2}(y_k^1)$ (otherwise the finite subcover would cover $K$). We will call this ``uncoverable'' open ball the ``bad ball''. Rearrange the $\{y_k^1\}$ to put the ``bad ball'' at the beginning, i.e. the ``bad ball'' is labeled $B_{1/2}(y_1^1)$. Let
\[
B_1 = B_{1/2}(y_1^1) \cap K.
\]
Note that $B_1$ cannot be covered by finitely many $U_\alpha$.
\item Repeat this for $\epsilon_2 = 1/2^2$. Again, since $K$ is totally bounded, we can find points $y_1^2, \dots, y_{n(2)}^2$ such that
\[
K \subset \bigcup_{k=1}^{n(2)} B_{1/2^2}(y_k^2).
\]
Since $B_1$ cannot be covered by finitely many $U_\alpha$, no finite subcover can cover at least one of the sets $B_1 \cap B_{1/2^2}(y_k^2)$. Again, rearrange the $\{y_k^2\}$ so that $B_{1/2^2}(y_1^2)$ is the ``bad ball'', and let
\[
B_2 = B_1 \cap B_{1/2^2}(y_1^2).
\]
Again, $B_2$ cannot be covered by finitely many $U_\alpha$.
\item Repeat this process with $\epsilon_n = 1/2^n$ to get a nested sequence of nonempty sets
\[
K \supset B_1 \supset B_2 \supset B_3 \supset \dots
\]
such that $B_n \subset B_{1/2^n}(y_1^n)$ for some $y_1^n \in K$, and none of the $B_n$ can be covered by finitely many $U_\alpha$. We note that each set $B_n$ is contained in a ball of radius $1/2^n$.
\item For each $n \in \N$, choose $x_n \in B_n$. Since the sets $\{B_n\}$ are nested, and each $B_n$ is contained in a ball of radius $1/2^n$, $\{x_n\}$ is a Cauchy sequence. Since $K$ is complete, $x_n \rightarrow x^* \in K$. Furthermore, $x^* \in B_n$ for all $n \in \N$.
\item Since $\{U_\alpha\}$ covers $K$, $x^* \in U_{\alpha_0}$ for some $\alpha_0$. Since $U_{\alpha_0}$ is open and the $B_n$ are nested, shrinking, and contain $x^*$, $B_n \subset U_{\alpha_0}$ for sufficiently large $n$, which contradicts the fact that no $B_n$ can be covered by finitely many $U_\alpha$.
\end{enumerate}
\end{proof}
\end{theorem}

As a corollary, closed subsets of compact sets are compact. 

\begin{corollary}
Let $K$ be a compact subset of a metric space $X$. If $A \subset K$ is closed, then $A$ is compact.
\end{corollary}
\begin{proof}
Let $\{x_n\}$ be a sequence in $A$. Then there is a subsequence $x_{n_k} \rightarrow x^* \in K$, since $K$ is sequentially compact. Since $A$ is closed, $x^* \in A$. It follows that $A$ is sequentially compact, thus $A$ is compact by the compact equivalence theorem.
\end{proof}

Next, we show that compact subsets of metric spaces are closed.

\begin{proposition}
Compact subsets of metric spaces are closed.
\begin{proof}
Let $K$ be a compact subset of metric space $X$. We will show that $X\setminus K$ is open, which implies that $K$ is closed. Choose any $x \in X\setminus K$, i.e. $x \neq K$. For all $y \in K$, let $r(y)$ be the distance $r(y) = \frac{1}{2}d(y, x) > 0$, since $y \neq x$. The collection of open balls $\{ B_{r(y)}(y) \}_{y \in K}$ is an open cover for $K$, and none of them contain $x$ by our definition of $r(y)$. By compactness, we can find a finite subcover $\{ B_{r(y_1)}(y_1), \dots, B_{r(y_n)}(y_n)\}$ of $K$. Let $r = \min\{ r(y_1), \dots, r(y_n) \}$. Then $B_r(x)$ does not intersect this finite subcover, which means that $B_r(x)$ lies outside of $K$. It follows that $B_r(x) \subset X\setminus K$, from which we conclude that $X\setminus K$ is open.
\end{proof}
\end{proposition}

Although it is nice to have three equivalent criteria for compactness in metric spaces, they are still annoying to check. Fortunately, we have a nice criterion for compactness in $\R^n$, which generalizes the idea that closed, bounded intervals are ``special''.

\begin{theorem}[Heine-Borel]
 A subset $K \subset \R^n$ is compact $\iff$ $K$ is closed and bounded.
\begin{proof}We prove both directions.

($\implies$) Since $\R^n$ is a metric space and $K$ is compact, $K$ is closed and totally bounded. Since a totally bounded set is bounded, $K$ is bounded.

($\impliedby$) Since $K$ is bounded, it fits inside a closed box $B$ in $\R^n$. Since $K$ is closed, and closed subsets of compact sets are compact, it suffices to show that $B$ is compact. We will show that $B$ is sequentially compact. Since $B$ is bounded, it follows from the Bolzano-Weierstrass theorem that any sequence in $B$ has a convergent subsequence, whose limit must be in $B$ since $B$ is closed. Since $B$ is sequentially compact, it is compact by the compact equivalence theorem.
\end{proof}
\end{theorem}

Next, we show that compactness is also a topological property, i.e. it is preserved by continuous functions. Before we do this, we recall two important relations involving $f$ and the inverse image operation $f^{-1}$.
\begin{enumerate}
    \item $f(f^{-1}(E)) \subset E$, with equality if $f$ is surjective.
    \item $f^{-1}(f(E)) \supset E$, with equality if $f$ is injective.
\end{enumerate}

\begin{theorem}
Let $f: (X, d_1) \rightarrow (Y, d_2)$ be continuous, and $K$ compact in $X$. Then $f(K)$ is compact in $Y$. In other words, continuous images of compact sets are compact.
\begin{proof}
Here, it makes sense to use the topological definition of compactness.
\begin{enumerate}
    \item Let $\{ U_\alpha \}_{\alpha \in \mathcal{A}}$ be an open cover of $f(K)$. \item Since $f$ is continuous, $f^{-1}(U_\alpha)$ is open in $X$, thus $\{ f^{-1}(U_\alpha) \}_{\alpha \in \mathcal{A}}$ is an open cover for $K$. 
    \item Since $K$ is compact, we can find a finite subcover $\{ f^{-1}(U_1), \dots, f^{-1}(U_n) \}$ for $K$. 
    \item Sending the finite subcover back through $f$, $\{ f(f^{-1}(U_1)), \dots, f(f^{-1}(U_n)) \}$ covers $K$.
    \item Since $f(f^{-1}(U_k)) \subset U_k$, $\{ U_1, \dots, U_n \}$ is a finite subcover for $f(K)$.
\end{enumerate}
\end{proof}
\end{theorem}

The extreme value theorem is a direct consequence of this theorem.

\begin{theorem}[Extreme Value Theorem]
Let $f: (X, d) \rightarrow \R$ continuous and $K \subset X$ compact. Then $f$ attains an absolute maximum and an absolute minimum on $K$.  
\end{theorem}
\begin{proof}
Since $K$ is compact, $f(K) \subset \R$ is compact, thus closed and bounded by Heine-Borel.
\end{proof}

Finally, we will prove a generalization of the theorem proved by Heine that continuity implies uniform continuity on a compact set. Uniform continuity is defined as follows.

\begin{definition}
A function $f: (X, d_1) \rightarrow (Y, d_2)$ is \textbf{uniformly continuous} if, for every $\epsilon > 0$, there exists $\delta > 0$ (dependent on $\epsilon$, but not on $x$) such that whenever $d_1(x, y) < \delta$, $d_2(f(x), f(y)) < \epsilon$.
\end{definition}
The main difference between uniform continuity and continuity is that, for a given $\epsilon > 0$, the same $\delta$ must work for all $x$, i.e. $\delta$ is independent of $x$. For simplicity, we will prove the uniform continuity theorem for real-valued functions, although it same result holds (with the same proof) for any pair of metric spaces.

\begin{theorem}[Uniform Continuity Theorem]
Let $f: (X, d) \rightarrow \R$ be continuous and let $K \subset X$ compact. Then $f$ is uniformly continuous on $K$.
\begin{proof}
We present two proofs here. The first is the standard constructive proof.
\begin{enumerate}
    \item Let $\epsilon > 0$. Since $f$ is continuous on $K$, for every $x_0 \in K$ we can find $\delta(x_0)$ such that $|f(x) - f(x_0)| < \epsilon/2$ whenever $d(x, x_0) < \delta(x_0)$. In general, the $\delta(x_0)$ will be different.
    \item The collection of open balls $\left\{ B_{\delta(x)/2}(x) \right\}_{x \in K}$ is an open cover for $K$. By compactness, we can find a finite subcover. In other words, we can find points $x_1, \dots, x_n \in K$ such that
    \[
    K \subset B_{\delta(x_1)/2}(x_1) \cup \dots \cup B_{\delta(x_n)/2}(x_n).
    \]
    \item Let $\delta = \min\{ \delta(x_1)/2, \dots, \delta(x_n)/2 \} > 0$. (We need this minimum to be well-defined and positive, which is why we need compactness to give us a finite subcover).
    \item Choose any $x, y \in K$ with $d(x, y) < \delta$. We will show that $|f(x) - f(y)| < \epsilon$.
    \item Because of the finite subcover, $x$ must be inside one of the finite set of open balls $B_{\delta(x_k)/2}(x_k)$. It follows that $d(x, x_k) < \delta(x_k)/2$ for some $k \in \{1, dots, n\}$.
    \item By the triangle inequality,
    \[
    d(y, x_k) \leq d(y, x) + d(x, x_k) < \delta + \delta(x_k)/2 \leq \delta(x_k),
    \]
    since $\delta \leq \delta(x_k)/2$.
    \item Finally, by the triangle inequality and the continuity of $f$,
    \[
    |f(y) - f(x)| \leq |f(y) - f(x_k)| + |f(x_k) - f(x)|
    \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon,
    \]
    since both $y$ and $x$ are within a distance $\delta(x_k)$ of $x_k$.
\end{enumerate}
Alternatively, we may use proof by contradiction.
\begin{enumerate}
    \item Suppose the conclusion is not true. Then for a specific $\epsilon > 0$, we can find sequences $\{x_n\}, \{y_n\} \subset K$ such that $d(x_n, y_n) \rightarrow 0$, but $|f(x_n) - f(y_n)| \geq \epsilon$.
    \item Since $K$ is compact, thus sequentially compact, $\{x_n\}$ has a convergent subsequence $x_{n_k} \rightarrow x^*$, where $x^* \in K$, since $K$ is closed.
    \item Since $d(x_n, y_n) \rightarrow 0$, $y_{n_k} \rightarrow x^*$ as well.
    \item By the triangle inequality,
    \[
    0 < \epsilon \leq |f(x_{n_k}) - f(y_{n_k})|
    \leq \underbrace{|f(x_{n_k}) - f(x^*)| + |f(x^*) - f(y_{n_k})|}_{\text{both  }\rightarrow\:0\text{ by continuity of }f} \rightarrow 0,
    \]
    which is a contradiction.
\end{enumerate}
\end{proof}
\end{theorem}

\section{Arzela-Ascoli theorem}

The Arzela-Ascoli theorem is the analogue of the Heine-Borel theorem for the metric space $C([a,b])$, the space of real-valued functions on $[a,b]$. It gives criteria for compactness that are (hopefully!) easier to check. We will prove the theorem on the more general space of real-valued, continuous functions on a compact subset $K$ of a metric space $(X,d)$. This space is denoted $C(K)$ (sometimes you will see $C^0(K)$ or $C(K, \R)$), and we use the supremum (maximum) metric, which is defined by
\[
d(f, g) = \max_{x \in K}|f(x) - g(x)|.
\]
Since $f$ and $g$ are continuous, and $K$ is compact, the maximum is well-defined by the Extreme Value Theorem. In addition, $C(K)$ is complete; the proof is identical to that for $C([a,b])$. Before we can state the theorem, we will need the following two definitions.

\begin{definition}
A subset $A \subset C(K)$ is \textbf{uniformly bounded} if there exists $M \geq 0$ such that
$|f(x)| \leq M$ for all $f \in A$ and $x \in K$. This can also be written as
\[
\sup_{f \in A, x \in K} |f(x)| \leq M.
\]
\end{definition}

Next, we define equicontinuity of a set of functions. This is essentially the same as uniform continuity, except the same $\delta$ must work for every function in the set.

\begin{definition}
A subset $A \subset C(K)$ is \textbf{equicontinuous} if, for every $\epsilon > 0$, there exists $\delta > 0$ (dependent on $\epsilon$, but not on $f$ or $x$) such that whenever $d(x,y) < \delta$, $|f(x) - f(y)| < \epsilon$ for all $f \in A$. Alternatively, we can write this as
\[
\sup_{f\in A}|f(x) - f(y)| \rightarrow 0 \text{ as } d(x,y) \rightarrow 0.
\]
\end{definition}

Before we can prove the theorem, we will need a result which is interesting in its own right. You will see the concept of separability (which is unfortunately named!) again in real analysis.

\begin{definition}
A metric space is \textbf{separable} if it contains a countable dense subset.
\end{definition}

Here are some examples of separable metric spaces.
\begin{enumerate}
    \item $\R$ contains $\Q$ as countable dense subset. In general $\R^n$ contains $\Q^n$ as a countable dense subset.
    \item $C([0,1])$ is separable. It follows from the Weierstrass Approximation Theorem and the fact that $\Q$ is countable that $\Q[x]$, the set of polynomials with rational coefficients, is a countable dense subset of $C([0,1])$.
\end{enumerate}

Next, we show that every compact metric space is separable.

\begin{proposition}
Every compact metric space $K$ contains a countable dense subset $S$.
\begin{proof}
Choose any positive integer $n$. Then the collection of open balls $\{ B_{1/n}(x)\}_{x \in K}$ with radius $1/n$ is an open cover for $K$. By compactness of $K$, we can find a finite collection of points $S_n = \{x_1^n, \dots, x_{k(n)}^n\} \subset K$, where $k(n)$ depends on $n$, such that
\[
K \subset \bigcup_{j = 1}^{k(n)}B_{1/n}(x_j^n).
\]
The set $S_n$ is a finite set of points in $K$, and every point in $K$ is ``$1/n$-close'' to a point in $S_n$. Perform this procedure for all $n \in N$, and take the union 
\[
S = \bigcup_{n=1}^\infty S_n.
\]
$S$ is dense in $K$, since, for any $\epsilon > 0$ and $x \in K$, every open ball $B_\epsilon(x)$ contains a point in $S$. $S$ is countable, since it is the countable union of finite sets.
\end{proof}
\end{proposition}

We can now state and prove the Arzela-Ascoli theorem.

\begin{theorem}[Arzela-Ascoli] Let $K$ be a compact subset of a metric space $(X, d)$. If a sequence of functions $\{f_n\} \in C(K)$ is uniformly bounded and equicontinuous, it has a uniformly convergent subsequence. Thus if a subset $A \subset C(K)$ is closed, uniformly bounded, and equicontinuous, it is compact.
\begin{proof}
Let $\{f_n\}$ be an equicontinuous and uniformly bounded, with uniform bound $M$. Since $K$ is a compact metric space, it is separable by the above proposition, therefore we can find a countable dense subset $S = \{ x_n \} \subset K$. The first step is to use a diagonal argument to find a subsequence of $\{f_n\}$ which converges pointwise on $S$. 
\begin{enumerate}
    \item We start by applying the sequence of functions $\{f_n\}$ to $x_1$. Consider the real-valued sequence $\{ f_n(x_1) \}$. Since $|f_n(x_1)| \leq M$ for all $n$, $\{ f_n(x_1) \}$ contains a convergent subsequence by Bolzano-Weierstrass. We will denote this convergent subsequence by $\{ f_{1,n}(x_1) \}$. The corresponding functions form a subsequence $\{ f_{1,n} \}$ of $\{f_n\}$.
    \item Next, we apply the sequence of functions $\{ f_{1,n} \}$ from the previous step to $x_2$, and form the real-valued sequence $\{ f_{1,n}(x_2) \}$. This sequence is also bounded by $M$, so it contains a convergent subsequence, which we will denote $\{ f_{2,n}(x_2) \}$. The corresponding functions $\{ f_{2,n} \}$ converge at both $x_1$ and $x_2$. (We note that $\{ f_{2,n} \}$is a subsequence of $\{ f_{1,n} \}$, which is in turn a subsequence of the original sequence $\{f_n\}$, i.e we have constructed a subsequence of a subsequence!) 
    \item Iterate this procedure to get a countable collection of subsequences of the original sequence $\{f_n\}$, which we can depict in the following grid.
    \begin{table}[H]
        \centering
        \begin{tabular}{cccc}
             $f_{1,1}$ & $f_{1,2}$ & $f_{1,3}$ & $\dots$  \\
             $f_{2,1}$ & $f_{2,2}$ & $f_{2,3}$ & $\dots$  \\
             $f_{3,1}$ & $f_{3,2}$ & $f_{3,3}$ & $\dots$ \\
             $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$
        \end{tabular}
    \end{table}
    By construction, each row in the grid is a subsequence of the previous row, and the sequence of functions in row $n$ converges at the points $x_1, \dots x_n$.
    \item Take the diagonal sequence $\{ g_n \} = \{ f_{n,n} \}$. This sequence is a subsequence of the original sequence $\{f_n\}$, and it converges at every point $x_n \in S$. (It turns out that we don't care what these limits actually are).
\end{enumerate}
We have constructed a subsequence $\{g_n\}$ of $\{f_n\}$ which converges on a dense subset of $K$. The last step is to show that $\{g_n\}$ converges uniformly on $K$.
\begin{enumerate}
    \item Let $\epsilon > 0$. By the definition of equicontinuity, we can find $\delta > 0$ such that whenever $d(x,y) < \delta$, $|g_n(x) - g_n(y)| < \epsilon$ for all $n \in \N$.
    \item Since $S = \{ x_n \}$ is dense in $K$, every point $x \in K$ lies inside $B_\delta(x_n)$ for some $x_n$. Thus the collection of open balls $\{ B_\delta(x_n) \}_{x_n \in S}$ is an open cover for $K$. Since $K$ is compact, we can find a finite subcover, i.e. there exists a positive integer $n_0$ such that
    \[
    K \subset \bigcup_{k = 1}^{n_0} B_\delta(x_k).
    \]
    Let $S_0 = \{ x_1, \dots, x_{n_0} \}$. Then for every $x \in K$, $d(x, x_k) < \delta$ for some $x_k \in S_0$. (Alternatively, we can construct $S_0$ directly by choosing $n$ such that $1/n < \delta$, covering $K$ with $(1/n)-$balls at each point, extracting a finite subcover, and letting $S_0$ be the set of the centers of the balls in the finite subcover).
    \item The sequence of functions $\{ g_n \}$ converges at the \emph{finite} set of points $S_0$. This means that each real-valued sequence $\{ g_n(x_k) \}$, for $k = 1, \dots n_0$, is convergent, and therefore is a Cauchy sequence. Since this is a finite set of sequences, we can find a positive integer $N$ such that for all $m, n \geq N$, $|g_m(x_k) - g_n(x_k)| <\epsilon$ for all $x_k \in S_0$.

    \item Choose any $x \in K$, and select $x_k \in S_0$ such that $d(x, x_k) < \delta$ according to step (2). For $m, n \geq N$, by the triangle inequality,
    \begin{align*}
    |g_n(x) - g_m(x)| &\leq |g_n(x) - g_n(x_k)| +
    |g_n(x_k) - g_m(x_k)| + |g_m(x_k) - g_m(x)| < 3\epsilon,
    \end{align*}
    where the first and third terms on the RHS are less than $\epsilon$ by equicontinuity, and the second term is less than $\epsilon$ by step (3). This implies that $\{g_n(x)\}$ is a (real-valued) Cauchy sequence for all $x$.
    \item Since $\{g_n(x)\}$ is a Cauchy sequence for all $x$, it follows from the completeness of $\R$ that $g_n(x) \rightarrow g(x)$ for all $x \in K$. Taking $m \rightarrow \infty$ above, we have
    \[
    |g_n(x) - g(x)| \leq 3\epsilon
    \]
    for all $x \in K$ and $n \geq N$. Since this is independent of $x$, the convergence is uniform.
    \end{enumerate}
We have successfully found a subsequence $\{g_n\}$ of $\{f_n(x)\}$ which converges uniformly on $K$. To prove the second statement of the theorem, let $A \subset C(K)$ be closed, uniformly bounded, and equicontinuous. By what we have already proved, any sequence $\{f_n\}\subset A$ must contain a convergent subsequence $f_{n_k} \rightarrow f$. Since $A$ is closed, the limit $f \in A$, so $A$ is sequentially compact. It follows that $A$ is compact by the compact equivalence theorem.
\end{proof}
\end{theorem}

We also have a partial converse to the Arzela-Ascoli theorem.
\begin{theorem} Let $K \subset (X, d)$ be compact. Then if $A \subset C(K)$ is totally bounded, it is uniformly bounded and equicontinuous.
\begin{proof}The idea behind the proof is to take an arbitrary function $f \in A$ and show that $f$ is bounded by a constant which is independent of $f$, and $f$ is uniformly continuous with $\delta$ depending on $\epsilon$ only, i.e. independent of $f$. We proceed as follows.
\begin{enumerate}
\item Let $\epsilon > 0$. Using the definition of totally bounded, there exists a finite collection of functions $f_1^{\epsilon}, \dots, f_{n(\epsilon)}^{\epsilon} \in A$, where the number of functions $n(\epsilon)$ depends on $\epsilon$, such that 
\[
A \subset \bigcup_{k=1}^{n(\epsilon)} B_{\epsilon}(f_k^{\epsilon}).
\]
The open balls $B_{\epsilon}(f_k^{\epsilon})$ are defined using the maximum (supremum) metric.
\item Choose any $f \in A$. Then $f \in B_{\epsilon} (f_k^\epsilon)$ for some $k \in \{1, \dots, n(\epsilon)\}$. In particular, this means that $\sup_{x \in K}|f(x) - f_k^\epsilon(x)| < \epsilon$.
\item First, we show uniform boundedness. By the triangle inequality
\begin{align*}
\sup_{x \in K}|f(x)| &\leq \sup_{x \in K}|f(x) - f_k^\epsilon(x)| +  \sup_{x \in K}|f_k^\epsilon(x)| \\
&\leq \underbrace{ \epsilon + \max_{j = 1, \dots, n(\epsilon)} \sup_{x \in K}|f_j^\epsilon(x)| }_{\text{define this to be $M$, which is independent of $f$}} \\
&= M < \infty,
\end{align*}
where $\sup_{x \in K}|f_j^\epsilon(x)|$ is finite by the extreme value theorem, and we are taking the maximum over a finite set. Since $M$ is independent of $f$, we conclude that $A$ is uniformly bounded.
\item Next, we show equicontinuity. Using the triangle inequality again,
\begin{align*}
|f(x) - f(y)| &\leq |f(x) - f_k^\epsilon(x)| + |f_k^\epsilon(x) - f_k^\epsilon(y)| + |f_k^\epsilon(y) - f(y)| \\
&\leq 2 \epsilon + \max_{j = 1, \dots, n(\epsilon)}|f_j^\epsilon(x) - f_j^\epsilon(y)|.
\end{align*}
\item By the uniform continuity theorem, each function $f_j^\epsilon(x)$ is uniformly continuous on $K$. This means that for all $j = 1, \dots, n(\epsilon)$, we can find $\delta_j > 0$ such that if $|x - y|<\delta_j$, $|f_j^\epsilon(x) - f_j^\epsilon(y)| < \epsilon$. Let $\delta = \min\{ \delta_1, \dots, \delta_{n(\epsilon)} \}$, which is positive since we are taking the minimum of a finite set of positive real numbers. Thus, whenever $|x - y| < \delta$,
\[
\max_{j = 1, \dots, n(\epsilon)}|f_j^\epsilon(x) - f_j^\epsilon(y)| < \epsilon.
\]
\item Combining the previous two steps, if $|x - y| < \delta$, then
\[
|f(x) - f(y)| < 3 \epsilon.
\]
Since $\delta$ is independent of $f$, we conclude that $A$ is equicontinuous.
\end{enumerate}
\end{proof}
\end{theorem}

We would like criteria which are easier to check than uniform boundedness and equicontinuity. Here are some results of that nature. First, we define Lipschitz continuity, which shows up in many contexts, including existence and uniqueness of solutions to ODEs. Again, we define this for real-valued functions, but the definition extends to any metric space.

\begin{definition}
A function $f: (X, d) \rightarrow \R$ is \textbf{Lipschitz continuous} (or just Lipschitz) if there exists a positive constant $L > 0$ (the Lipschitz constant) such that for all $x, y \in X$,
\[
|f(x) - f(y)| \leq L d(x, y).
\]
\end{definition}
It is not difficult to show that Lipschitz continuity implies uniform continuity. For function from $\R$ to $\R$, the Lipschitz condition reduces to $|f(x) - f(y)| \leq L|x-y|$. This is trivially true for $x = y$, thus for $x \neq y$ we can divide both sides by $|x - y|$ to obtain the condition
\[
\frac{|f(x) - f(y)|}{|x-y|} \leq L.
\]
Intuitively, the Lipschitz constant $L$ puts a bound on the slopes of all of the possible secant lines of $f$. A Lipschitz function is ``nice'' in the sense that it ``does not change too fast''. If $f: \R \rightarrow \R$ is continuously differentiable (which is denoted $C^1$) and $f'(x)$ is bounded by $L$, it follows from the mean value theorem that $f$ is Lipschitz continuous with constant $L$. 

\begin{lemma}Suppose $f: \R\subset \R \rightarrow \R$ is continuously differentiable, and $|f'(x)| \leq L$. Then $f$ is Lipschitz continuous, with Lipschitz constant $L$. 
\end{lemma}
\begin{proof}
Choose any distinct points $x$ and $y$ in $\R$. Then by the mean value theorem, there exists a point $c$ between $x$ and $y$ such that
\[
\frac{f(x)-f(y)}{x-y} = f'(c).
\]
Taking absolute values and rearranging, this becomes
\[
|f(x)-f(y)| = |f'(c)||x-y| \leq L |x-y|,
\]
which is the desired result.
\end{proof}

In particular, this holds when $f'(x)$ is continuous and we restrict ourselves to a closed interval $[a, b]$, in which case the Lipschitz constant is $L = \max_{x\in[a,b]} |f'(x)|$. In the next lemma, we give a criterion for equicontinuity in terms of Lipschitz continuity.

\begin{lemma}
Let $K \subset \R^d$ be compact and $A \subset C(K)$. If every function in $A$ is Lipschitz continuous with the same Lipschitz constant $L$, then $A$ is equicontinuous.
\begin{proof}
Let $\epsilon > 0$, and choose $\delta = \epsilon/2L$. Then for all $x, y \in K$ with $|x-y| < \delta$ and for all $f \in A$,
\[
|f(x) - f(y)| \leq L|x-y| \leq L\delta = \frac{\epsilon}{2} < \epsilon.
\]
\end{proof}
\end{lemma}

The same result holds for H\"{o}lder continuous functions. H\"{o}lder continuity is a weaker condition than Lipschitz continuity, and it shows up in the study of PDEs.

\begin{definition}
A function $f: \R^d \rightarrow \R$ is \textbf{H\"{o}lder continuous} with exponent $\alpha \in (0, 1]$ (sometimes this is called $\alpha$-H\"{o}lder continuous) if there exists a positive constant $C > 0$ such that for all $x, y \in X$,
\[
|f(x) - f(y)| \leq L |x - y|^\alpha.
\]
\end{definition}

The case $\alpha = 1$ is Lipschitz continuity. We exclude $\alpha > 1$, because it can be shown that any function on an interval $[a, b]$ satisfying the H\"{o}lder condition with $\alpha > 1$ is constant. H\"{o}lder continuity also implies uniform continuity, and the result of the previous lemma holds if every function in $A$ is H\"{o}lder continuous with the same $\alpha$ and $C$. We mentioned above that the mean value theorem implies that if $f: [a, b] \rightarrow \R$ is continuously differentiable, $f$ is Lipschitz with Lipschitz constant $\max_{x\in [a,b]}|f'(x)|$.  Unfortunately, this does not work in higher dimensions, since there is no $n-$dimensional analogue to the mean value theorem.  We can obtain a similar result, however, if $f$ is continuously differentiable on a compact, convex set. We recall that since $f'$ is continuous, it is bounded on all compact sets. First, we define a convex set.

\begin{definition}
A set $E$ in $\R^d$ is \textbf{convex} if for all $x, y \in E$, the line segment joining them is also in $E$. In other words, for all $x, y \in E$,
\begin{align*}
tx + (1 - t)y &\in E && t \in [0, 1].
\end{align*}
\end{definition}

We then have the following lemma.

\begin{lemma}
Let $K \subset \R^d$ convex and compact, and $K \subset U \subset \R^d$, where $U$ is open. Let $f: U \rightarrow \R$ be continuously differentiable, and let $\sup_{x \in K} ||Df(x)|| = L$. Then $f: K \rightarrow \R$ is Lipschitz with constant $L$.
\end{lemma}

\begin{proof}
Let $x,y \in K$. Since $K$ is convex, $tx + (1 - t)y \in K$ for all 
$t \in [0,1]$. Next, we use the fundamental theorem of calculus to write $f(x)- f(y)$ in the following integrated form:
\begin{align*}
f(x)- f(y) &= \int_0^1 \frac{d}{dt} f\left(tx + (1-t)y\right) dt \\
&= \int_0^1 Df(tx + (1-t)y) \cdot (x-y) dt \\
&= \left( \int_0^1 Df(tx + (1-t)y) dt \right) \cdot (x-y),
\end{align*}
where the second line follows from the multivariable chain rule, and the dot represents the dot product in $\R^d$. Taking absolute values, we have

\begin{align*}        
|f(x)- f(y)| &\leq \left| \int_0^1 Df(tx + (1-t)y) dt \right| |x-y| \\
&\leq \left( \int_0^1 \underbrace{ || Df(tx + (1-t)y) || }_{\leq\:L \text{ since } tx + (1 - t)y\:\in\:K } dt \right) |x-y| \\
&\leq L |x-y|.
\end{align*}
    
\end{proof}

\subsection{Application to Existence of Solutions to ODEs}

In this section, we will use the Arzela-Ascoli theorem to prove the existence of solution to ODEs. Before we get too theoretical, let's look at several examples of ODEs on $\R$.

\begin{enumerate}
\item Exponential growth/decay:
\begin{align*}
\frac{du}{dt} &= k u \\
u(0) &= u_0 
\end{align*}
By separation of variables, this has solution
\[
u(t) = u_0 e^{kt}
\]
which exists for all time $t$.

\item ``Superexponential'' growth:
\begin{align*}
\frac{du}{dt} &= u^2 \\
u(0) &= 1
\end{align*}
By separation of variables, this has solution
\[
u(t) = \frac{1}{1 - t}
\]
Since we start at $t = 0$, $u(t) \rightarrow \infty$ as $t \rightarrow 1$. In other words, the solution blows up in finite time.

\item Non-uniqueness:
\begin{align*}
\frac{du}{dt} &= u^{1/3} \\
u(0) &= 0
\end{align*}
We can see by inspection that $u(t) = 0$ is a solution. By separation of variable, we can also find another solution.
\begin{align*}
u(t) &= \left( \frac{2t}{3} \right)^{3/2} && t \geq 0
\end{align*}
We actually have an infinite family of solutions, given piecewise by
\[
u(t) = \begin{cases}
0 & t \leq T \\
\left( \frac{2(t-T)}{3} \right)^{3/2} & t > T
\end{cases}
\]
where $T \geq 0$. In other words, we start at the zero solution and at time $T$ we start following the nonzero solution. (This piecewise function is continuously differentiable but not smooth).
\end{enumerate}

With these examples in hand, we can discuss existence of solutions to ODEs. From the second example, we will in general only be able to show local existence, i.e. existence in an interval around the starting point. The most basic result is due to Peano, where only continuity is assumed. 

\begin{theorem}[Cauchy-Peano Existence Theorem]
Consider the initial value problem on $\R^n$
\begin{align*}
\frac{du}{dt} &= f(t, u) \\
u(t_0) &= u_0
\end{align*}
If $f$ is continuous in a neighborhood of $(t_0, u_0)$, then there exists at least one solution $u(t)$ defined in a neighborhood of $t_0$. There is no guarantee of uniqueness.
\begin{proof}
For simplicity, we will only consider $f: \R \rightarrow \R$. For convenience and without loss of generality (since we can always translate the function), we will take $t_0 = 0$ and $u_0 = 0$. The strategy of the proof is:
\begin{enumerate}
    \item Rewrite the problem in integral form.
	\item Construct a sequence of approximate solutions $\{ u_n(t) \}$ using the forward Euler method, where the mesh size $h \rightarrow 0$ as $n \rightarrow \infty$.
	\item Show this sequence $\{ u_n(t) \}$ is uniformly bounded and equicontinuous.
	\item By Arzela-Ascoli, $\{ u_n(t) \}$ must have a subsequence which converges to $u(t)$.
	\item Show that the limit $u(t)$ is what we want.
\end{enumerate}
We proceed as follows.
\begin{enumerate}
    \item Since $f$ is continuous in a neighborhood 0, $f$ is continuous on a box $B = [-R, R] \times [-R, R]$ for $R > 0$. Since $f$ is continuous, $|f(t, u)| \leq M$ on $B$ for some $M \geq 1$.
    \item Next, we rewrite the problem in integral form. This is necessary because we will use piecewise linear functions as approximate solutions, and these are not differentiable at the points where the pieces join together. It is not hard to show that $u(t)$ satisfies the original initial value problem if and only if $u(t)$ satisfies the integral equation
    \[
    u(t) = u_0 + \int_{t_0}^t f(s, u(s))ds
    \]
    Since we take $t_0 = 0$ and $u_0 = 0$, this becomes
    \[
    u(t) = \int_0^t f(s, u(s))ds
    \]
    \item Let $T = R/M$, and consider the sequence of Euler approximations $u_n(t)$ on $[-T, T]$, which are defined as follows. Rather than write formulas for the $u_n(t)$, which is annoying, we will give the procedure used to construct them.
    \begin{enumerate}
        \item Take $h_n = T/n$ be the grid mesh size, so that the grid for $t$ is given by
        \[
        [-t_n^n, -t_{n-1}^n, \dots, -t_1^n, 0, t_1^n, t_2^n, \dots, t_n^n] = [-n h_n, -(n-1)h_n, \dots, -h_n, 0, h_n, 2 h_n, \dots, n h_n ] 
        \]
        which contains $2n + 1$ grid points, and $n h_n = T$.
        \item We start with initial condition $u_0^n = 0$, then compute $u_n$ on the rest of the grid using the forward Euler method.
        \begin{align*}
            u_0^n &= 0 \\
            u_1^n &=  0 + f(0, 0) h_n \\
            u_2^n &=  u_1^n + f(t_1^n, u_1^n) h_n \\
            & \vdots \\
            u_{m+1}^n &= u_m^n + f(t_m^n, u_m^n) h_n
        \end{align*}
        and similarly for going backwards. 
        \item $u_n(t)$ is the piecewise linear interpolation of these grid values. Since $u_n(t)$ is piecewise linear, $u_n(t)$ is differentiable on the interval $[t_m^n, t_{m+1}^n]$ with
        \[
        u_n'(t) = f(t_m^n, u_m^n)
        \]
        thus on each interval, $|u_n'(t)| \leq M$.
    \end{enumerate}
    
    \item Next, we show that the sequence $\{ u_n(t) \}$ is uniformly bounded and equicontinuous. For uniform boundedness, we note that for each Euler step, we have the bound
    \[
    |u_{m+1}^n - u_m^n| \leq M h_n
    \]
    Since $u_n(t)$ involves $n$ steps Euler in each direction, and $u_n(t)$ is linear between steps, we have the bound
    \[
    |u_n(t)| \leq n M h_n = M T
    \]
    For equicontinuity, since $u_n(t)$ is differentiable everywhere except at a finite number of grid points, we have
    \begin{align*}
        |u_n(t) - u_n(s)| &\leq \int_s^t |u_n'(r)| dr \\
        &\leq \int_s^t M dr = M|t-s|
    \end{align*}
    where we evaluate the integral by splitting it up into pieces at the grid points. Since $\{u_n(t) \}$ are Lipschitz with the same Liptshitz constant $M$, the sequence $\{ u_n(t) \}$ is equicontinuous. 
    
    \item By Arzela-Ascoli, $\{u_n(t) \}$ has a uniformly convergent subsequence, which we will denote $\{ v_k(t) = u_{n_k}(t) \}$. Let $v_k(t) \rightarrow u(t)$ uniformly. 
    
    \item The only thing left is to show that $u(t)$ is a solution to the integrated form of the ODE. Using the triangle inequality
    \begin{align*}
    &\left| u(t) - \int_0^t f(s, u(s))ds \right| \\
    &\,\leq |u(t) - v_k(t)| + \left| \int_0^t f(s, v_k(s))ds - \int_0^t f(s, u(s))ds \right| + \left| v_k(t) - \int_0^t f(s, v_k(s))ds \right|
    \end{align*}
    The first term on the RHS $\rightarrow 0$ by uniform convergence of $\{ u_n(t) \}$ from Arzela-Ascoli. For the second term on the RHS,
    \[
    \left| \int_0^t f(s, v_k(s))ds - \int_0^t f(s, u(s))ds \right|  
    \leq \int_0^t | f(s, v_k(s))ds - f(s, u(s))| ds
    \]
    which also $\rightarrow 0$ by uniform convergence of $v_k(t)$ and uniform continuity of $f$ on the closed interval $[-T, T]$.
    
    \item All that remains is to show the third term on the RHS $\rightarrow 0$. This is unfortunately really annoying to do, since it quickly becomes a subscript nightmare. The idea is as follows. Since we only care about the subsequence $\{v_k(t)\}$, we will always have $n = n_k$, so for convenience we will denote the grid points by $t_j^k$, where $t_j^k = t_j^{n_k}$. Let $t \in [0, T]$. Then $t$ is between two grid points, so $t \in [t_m^k, t_{m+1}^k]$ for some $m$. (If $t$ is actually one of the grid points, take $t = t_{m+1}^k$). 
    
    \item Let $v_j^k$ be the values of $v_k(t)$ on the grid $t_j^k$. Then we can write $v_k(t)$ as a telescoping sum involving grid points
    \[
    v_k(t) = \sum_{j=0}^{m-1} (v_{j+1}^k - v_j^k) + (v_k(t) - v_m^k)
    \]
    where we recall that $v_k(0) = 0$. Substituting this in, splitting the integral up along the grid, and using the FTC, this becomes
    \begin{align*}
    v_k(t) &- \int_0^t f(s, v_k(s))ds \\
    &= \sum_{j=0}^{m-1} (v_{j+1}^k - v_j^k) - \sum_{j=0}^{m-1} \int_{t_j^k}^{t_{j+1}^k} f(s, v_k(s))ds + (v_k(t) - v_m^k) - \int_{t_m^k}^t f(s, v_k(s))ds \\
    &= \sum_{j=0}^{m-1} \int_{t_j^k}^{t_{j+1}^k} v_k'(s, v_k(s))ds - \sum_{j=0}^{m-1} \int_{t_j^k}^{t_{j+1}^k} f(s, v_k(s))ds + \int_{t_m^k}^t v_k'(s, v_k(s))ds - \int_{t_m^k}^t f(s, v_k(s))ds \\
    &= \sum_{j=0}^{m-1} \int_{t_j^k}^{t_{j+1}^k} ( f(t_j^k, v_j^k) - f(s, v_k(s))) ds + \int_{t_m^k}^t f(t_m^k, v_m^k) - f(s, v_k(s)))ds
    \end{align*}
    Taking absolute values, we get
    \begin{align*}
    & \left| v_k(t) - \int_0^t f(s, v_k(s))ds \right| \\
    &\leq \sum_{j=0}^{m-1} \int_{t_j^k}^{t_{j+1}^k} | f(t_j^k, v_j^k) - f(s, v_k(s))| ds + \int_{t_m^k}^t |f(t_m^k, v_m^k) - f(s, v_k(s))|ds \\
    &\leq \sum_{j=0}^{m} \int_{t_j^k}^{t_{j+1}^k} | f(t_j^k, v_j^k) - f(s, v_k(s))| ds
    \end{align*}
    \item To estimate this nasty integral, we use the uniform continuity of $f$ on $B$. Let $\epsilon > 0$. Then we can find $\delta > 0$ such that if $\max(|s - t|, |u - v|) < \delta$, $|f(s, u) - f(t, v)| < \epsilon$. Since as $n$ increases we are refining the grid, choose $k$ sufficiently large so that $h_{n_k} < \delta / M$. Then we have for all $s \in [t_m^k, t_{m+1}^k]$,
    \[
    |s - t_m^k| \leq |t_{m+1}^k - t_m^k| < \delta
    \]
    and
    \[
    |v_k(s) - v_m^k| \leq  |v_{m+1}^k - v_m^k| \leq M h_{n_k} < \delta
    \]
    since $v_k(s)$ is a piecewise interpolation between $v_k^k$ and $v_{m+1}^k$. Then for all the integrands involved in the sum, we have the bound
    \[
    | f(t_j^k, v_j^k) - f(s, v_k(s)) | \leq \epsilon
    \]
    Putting all of this together, we have
    \begin{align*}
    \left| v_k(t) - \int_0^t f(s, v_k(s))ds \right| 
    &\leq \sum_{j=0}^{m} \int_{t_j^k}^{t_{j+1}^k} \epsilon ds \\
    &\leq \epsilon \sum_{j=0}^m (t_{j+1}^j - t_j^k) ds \\
    &\leq \epsilon \sum_{j=0}^m h_{n_k} \epsilon ds \\
    &\leq \epsilon (m+1) h_{n_k} \\
    &\leq \epsilon n_k \frac{T}{n_k} \\
    &= \epsilon T
    \end{align*}
    since $m$ is at most $n_k - 1$.
\end{enumerate}
Since the solution we obtained is only guaranteed to exist on $[-T, T]$, we call $u(t)$ a local solution to the initial value problem.
\end{proof}
\end{theorem}


We can extend this result to uniqueness by adding the condition that the function $f$ is Lipschitz. In order to do this, we need a very useful but technical result known as the Gronwall inequality. At first glance, this inequality appears to be the most useless result; why are we bounding a function by something which is growing exponentially? Taking a closer look, here is essentially what we are doing. We are taking a function $u(t)$ which is bounded by an exponential involving itself and obtaining a bound which involves an exponential independent of itself. There are seemingly hundreds of versions of the Gronwall inequality. (I think there is a whole book on them.) Here is one version.

\begin{lemma}[Gronwall Inequality]
Let $u(t), g(t)$ be non-negative, real-valued functions defined on $t \in [t_0, t_1]$. Let $C \geq 0$ be a constant so that
\begin{align*}
u(t) \leq C + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
Then
\begin{align*}
u(t) &\leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
It follows that if $C = 0$, $u = 0$ for $t \in [t_0, t_1]$.
\begin{proof}
\begin{enumerate}
\item We first consider the case where $C > 0$. Let
\[
v(t) = C + \int_{t_0}^t g(s) u(s) ds
\]
Then $u(t) \leq v(t)$ on $[t_0, t_1]$ (by assumption), and $v(t) \geq C > 0$ on $[t_0, t_1]$ (since $g, u \geq 0$). Differentiating $v$ with respect to $t$, we have
\[
v'(t) = g(t) u(t) \leq g(t)v(t)
\] 
Since $v(t) > 0$ on $[t_0, t_1]$, we can divide by it to get
\begin{align*}
\dfrac{v'(t)}{v(t)} &\leq g(t) && t \in [t_0, t_1]
\end{align*}
Now, we integrate $g(s)$.
\begin{align*}
\int_{t_0}^t g(s) ds &\geq \int_{t_0}^t \dfrac{v'(s)}{v(s)} ds \\
&= \int_{t_0}^t \frac{d}{ds}\left( \log {v(s)} \right) ds \\
&= \log\dfrac{v(t)}{v(t_0)}\\
&= \log\dfrac{v(t)}{C}
\end{align*}
Exponentiate both sides and multiply by $C$ to get
\begin{align*}
u(t) &\leq v(t) \leq C \exp \left( \int_{t_0}^t g(s) ds \right) && t \in [t_0, t_1]
\end{align*}
\item If $C = 0$, then for any $\epsilon > 0$,
\begin{align*}
u(t) \leq \epsilon + \int_{t_0}^t g(s) u(s) ds && t \in [t_0, t_1]
\end{align*}
By step 1, for $t \in [t_0, t_1]$ 
\begin{align*}
u(t) &\leq \epsilon \exp \left( \int_{t_0}^t g(s) ds \right) \\&\leq \epsilon \exp \left( \int_{t_0}^{t_1} g(s) ds \right) \\
&\leq M \epsilon
\end{align*}
which is independent of $t$. (To get the second line, we use the fact that $g(s)$ is nonnegative). Since $\epsilon$ is arbitrary, we conclude that $u = 0$ on $[t_0, t_1]$.
\end{enumerate}
\end{proof}
\end{lemma}

We can use the Gronwall inequality to prove uniqueness of solutions to initial value problems.

\begin{theorem}[Local Uniqueness of Solutions]
\begin{proof}
Suppose that two functions $u_1(t)$ and $u_2(t)$ are
local solutions to the initial value problem

Then since both $u_1$ and $u_2$ are solutions to the integrated form of the problem, 
\begin{align*}
|u(t)| &= \left| \left( u_0 + \int_0^t f(s, u_1(s))ds \right) - \left( u_0 + \int_0^t f(s, u_2(s))ds \right)  \right| \\
&= \int_0^t | f(s, u_1(s)) - f(s, u_2(s)) | ds \\
&\leq \int_0^t L |u_1(s) - u_2(s)| ds \\
&= 0 + \int_0^t L u(s) ds
\end{align*}
We have satisfied the conditions of the Gronwall inequality with $g(t) = L$ and $C = 0$. It follows that $u(t) = 0$ for $t \in [0, T]$, from which we conclude that $u_1(t) = u_2(t)$ for $t \in [0, T]$. We can similarly obtain the result for $t \in [-T,0]$.
\end{proof}
\end{theorem}

We have successfully obtained conditions for local existence and local uniqueness for solutions to initial value problems. We will return to this topic and obtain more uniform results when we discuss contraction mappings.

\section{Normed Vector Spaces and Banach Spaces}

First, we review the definition of a normed vector space.

\begin{definition}
A \emph{norm} on a vector space $V$ is a function $|| \cdot || : V \rightarrow \R$ with the following properties:
\begin{enumerate}
\item $|| x || \geq 0$
\item $||x|| = 0 \iff x = 0$
\item $||\lambda x|| = |\lambda|  ||x||$ for all scalars $\lambda$
\item $||x+y|| \leq ||x|| + ||y||$ (triangle inequality)
\end{enumerate}
A vector space paired with a norm is a \emph{normed vector space}.
\end{definition}

Every normed vector space is a metric space, since a norm induces a metric, which is given by
\[
d(x,y) = ||x-y||
\]
The converse, however, is not true. There are vector spaces (such as $C(\R)$) on which there is a metric, but no norm can be found.

Next, we define a bounded linear map between normed vector spaces. This is different from the usual concept of a bounded function that we use in, say, the Bolzano-Weierstrass theorem.

\begin{definition}
Let $X, Y$ be normed vector spaces with norms $||\cdot||_X$, $||\cdot||_Y$, respectively. Let $L: X \rightarrow Y$ be a linear map. Then $L$ is \textbf{bounded} if there exists a constant $C \geq 0$ such that for all $u \in X$,
\[
||Lu||_Y \leq C ||u||_X
\]
\end{definition}

In a normed vector space, boundedness and continuity of linear operators is equivalent, which we show in the next proposition.

\begin{proposition}
Let $X, Y$ normed vector spaces. Then a linear operator $L: X \rightarrow Y$ is bounded if and only if $L$ is continuous.
\begin{proof}
If $L$ is bounded, then 
\[
||Lu - Lv|| = ||L(u - v)|| \leq L ||u - v||
\]
ans so $L$ is Lipschitz, thus continuous. For the other direction, assume $L$ is continuous. Then since $L$ is continuous at 0, taking $\epsilon = 1$, we can find $\delta > 0$ such that for all $u$ with $||u||_X < \delta$
\[
||Lu||_Y \leq 1
\]
Thus for all $u$ with $||u||_X \leq 1$. Let $x \neq 0 \in X$. Then 
\[
\left|\left| \frac{\delta}{||x||}x \right|\right|_X \leq \delta
\]
from which it follows that 
\[
\left|\left| L\left( \frac{\delta}{||x||}x \right) \right|\right|_Y \leq 1
\]
Using the properties of the norm and the linearity $L$, we can rearrange this to get
\[
||L x||_Y \leq \frac{1}{\delta} ||x||_X
\]
\end{proof}
\end{proposition}

Let $\mathcal{L}(X, Y)$ be the space of bounded linear maps from $X$ to $Y$. If $X = Y$, we usually denote this $\mathcal{L}(X)$. Then we define the operator norm of $L \in \mathcal{L}$ as follows.

\begin{definition}
Let $L: X \rightarrow Y$ be a bounded linear operator. Then the \emph{operator norm} of $L$ is defined as one of the following, all of which are equivalent.
\begin{enumerate}
\item 
\[
||L|| = \sup_{u \neq 0, ||u||_X \leq 1} ||Lu||_Y 
\]
\item 
\[
||L|| = \sup_{||u||_X = 1} ||Lu||_Y 
\]
\item
\[
||L|| = \inf\{ C \geq 0: ||Lu||_Y \leq C ||u||_X \text{ for all } u \in X \}
\]
\end{enumerate}
\end{definition}

\begin{definition}
A \emph{Banach space} is a complete normed vector space, where completeness is with respect to the metric induced by the norm.
\end{definition}

We can show that the space $\mathcal{L}(X, Y)$ is a normed vector space using the operator norm. In addition, as long as $Y$ is a Banach space, $\mathcal{L}(X, Y)$ is a Banach space.

\begin{proposition}
$\mathcal{L}(X, Y)$ is a normed vector space with the operator norm. If $Y$ is a Banach space, then $\mathcal{L}(X, Y)$ is also a Banach space.
\begin{proof}
The proof is left as an exercise. To show $\mathcal{L}(X, Y)$ is complete, recall the proof for the completeness of $C([a,b])$.
\end{proof}
\end{proposition}

The next lemma is incredibly useful and gives a criterion for a certain linear operator on a Banach space to be invertible with bounded inverse. I use this all the time in my research (no joke!)

\begin{lemma}[Neumann Series]
Let $X$ be a Banach space, and let $S \in \mathcal{L}(X)$ with $||S|| < 1$. Then $I - S$ is invertible, and $(I - S)^{-1} \in L(X)$, where $I$ is the identity operator on $X$.
\end{lemma}

\begin{proof}
Define the \emph{Neumann series} for $S$ as 
\[
L = \sum_{n=0}^{\infty} S^n = I + S + S^2 + ... 
\]
which is the operator analogue of the normal geometric series. To show that this is well-defined, we note that the sequence of partial sums of $L$ is a Cauchy sequence, thus the sum converges since $X$ is complete. In addition, $L$ is bounded,
with
\[
||L|| \leq \sum_{n=0}^\infty ||S||^n \leq \frac{1}{1 - ||S||}
\]
Finally, since
\[(I - S)L = \underbrace{(I - S) \sum_{n=0}^{N} S^n}_{\rightarrow (I-S)L} = \sum_{n=0}^N (S^n - S^{n+1}) = 
\underbrace{I - S^{N+1}}_{\rightarrow I}
\]
$(I-S)L = I$. Similarly, $L(I - S) = I$.
\end{proof}

\section{Differentiation in Banach Spaces}

In this section, we extend the concept of differentiation to general Banach spaces. To do that, we will look at derivatives of functions on $\R$ in a different way.

From calculus, the derivative of a function $f: \R \rightarrow \R$ at $x = a$ is defined as 
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
provided the limit exists. We can rearrange this to get
\[
\lim_{h \rightarrow 0}\frac{f(a + h) - f(a) - f'(a) h }{h} = 0
\]
Note that $f'(a)$ is a real number, and as such is (trivally) a linear operator on $\R$. The term $f'(a) h$ can be thought of as applying this linear operator to $h$.

The original definition of the derivative makes no sense in higher dimensions (unless we are taking a directional derivative, in which case $h$ is still a real number). The second definition, however, is easily extended to higher dimensions.

\begin{definition}
A function $f:\R^n \rightarrow \R^m$ is \emph{differentiable} at $a \in R^n$ if there exists a linear transformation $L: \R^n \rightarrow \R^m$ such that
\[
\lim_{h \rightarrow 0}\frac{|f(a + h) - f(a) - L h |}{|h|} = 0
\]
\end{definition}

If all partial derivatives of $f$ exist and are continuous in a neighborhood of $a$, then $f$ is differentiable, and the derivative is given by the Jacobian matrix
\[
DF(a) = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n} \\
\vdots && \vdots \\
\frac{\partial f_m}{\partial x_m} & \dots & \frac{\partial f_1}{\partial x_n} 
\end{pmatrix}_{x = a}
\]
There are analogues of the chain rule and the Taylor theorem in higher dimensions. 

Finally, we extend this definition to Banach spaces.

\begin{definition}
Let $X, Y$ be Banach spaces, $U \subset X$ open, and $f \colon U \rightarrow Y$. Then $f$ is \textbf{differentiable} at $u \in U$ if there exists $L \in \mathcal{L}(X, Y)$ such that
\[
\lim_{h \rightarrow 0} \frac{ ||f(u + h) - f(u) - Lh ||_Y}{||h||_X} = 0.
\]
This is sometimes called the Fr\'{e}chet derivative. If f is differentiable at $u_0 \in U$, we use the notation $Df(u_0)$ or $f_u(u_0)$.
\end{definition}

It is not hard to show that that the Fr\'{e}chet derivative, if it exists, is unique. If $f$ is differentiable for all $u \in U$, then the map $Df\colon U \rightarrow \mathcal{L}(X,Y)$ defined by $u \rightarrow Df(u)$ is well-defined. A function $f$ is $C^1$ if this map is continuous.

Finally, we note the following regarding the Fr\'{e}chet derivative.

\begin{enumerate}
    \item The chain rule remains valid.
    \item High-order derivatives can be defined by considering the differentiability of $Df: U \rightarrow \mathcal{L}(X,Y)$, etc.
\end{enumerate}

The way this usually works is that you choose a candidate for the derivative and then use the defintion to show that your candidate works.

\section{Fixed Point Theorems}

In this section, we will look at fixed point theorems, which guarantee the existence of a unique fixed point of a function, i.e. a unique $x$ such that $f(x) = x$. As motivation, we will look at Newton's method, which is used to find isolated zeros of a differentiable, real-valued function $f(x)$. Newton's method works as follows.

\begin{enumerate}
\item Start with an initial guess $x_0$ (ideally near an actual zero of $f$) with $f'(x_0) \neq 0$.
\item Iterate the algorithm 
\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]
The new value of $x$ is where the tangent line of $f$ at $(x_n, f(x_n)$ hits the $x-$axis.
\item ????
\item PROFIT
\end{enumerate}

We would like this method to converge to a unique zero of $f$, which we hope is the zero near the initial guess. Define a ``Newton function'' $g$ by

\[
g(x) = x - \frac{f(x)}{f'(x)}
\]

If $x$ is a fixed point of $g$, i.e. $g(x) = x$, then (as long as $f'(x) \neq 0$), $f(x) = 0$, which is what we want. We would like to show that a fixed point of $g$ exists, and that Newton's Method converges to it. It turns out that criterion we will need for this to work is that the map $g$ is a contraction.

\begin{definition}Let $D$ be a subset of a normed vector space $(X, ||\cdot||)$. Then $F: D \rightarrow D$ is a \emph{contraction} if there exists a constant $L$ with $0 < L < 1$ such that 
\[
||F(x) - F(y)|| \leq L ||x - y||
\]
for all $x, y \in D$. In other words, $F$ is Lipschitz with Lipschitz constant $L < 1$.
\end{definition}

We can now state the Banach Fixed Point Theorem, which is also called the Contraction Mapping Principle. This theorem not only gives criteria for the existence of a unique fixed point, but also tells that that we can find it using successive iteration, just as in Newton's method. 

\begin{theorem}[Banach Fixed Point Theorem] Let $(X, ||\cdot||)$ be a Banach space, and $D \subset X$ a closed, nonempty subset of $X$. Suppose that the map $F: D \rightarrow D$ is a contraction, i.e. there exists a positive constant $L < 1$ such that for all $u, v \in D$,
\[
||F(u) - F(v)|| \leq L||u - v||
\]
Then $F$ has a unique fixed point $u^*$ in $D$, i.e. there exists a unique $u^* \in D$ such that $F(u^*) = u^*$.

\begin{proof}
As with many proofs involving complete metric spaces, the idea is to construct a Cauchy sequence in $D$ which must then converge since $X$ is complete. Then since $D$ is closed, the limit must be in $D$. We construct the sequence by iterating the map $F$ from any point in $D$.

\begin{enumerate}
	\item Start with an arbitrary $u_0 \in D$ and keep applying $F$. For all $n \geq 1$, define $u_n = F(u_{n-1})$. Since $F: D \rightarrow D$, $u_n \in D$ for all $n$.
	
	\item Since $F$ is a contraction,
	\begin{align*}
	||F(u_1) - F(u_0)|| &\leq L ||u_1 - u_0|| \\
	||F(u_2) - F(u_1)|| &\leq L ||u_2 - u_1|| = L||F(u_1) - F(u_0)|| \leq L^2 ||u_1 - u_0||
	\end{align*}
	Repeating this, we get a sequence $\{u_n\}$ with
	\[
	||F(u_n) - F(u_{n-1})|| \leq L^n ||u_1 - u_0||
	\]
	Using the definition of $F$, this becomes
	\[
	||u_{n+1} - u_n || \leq L^n ||u_1 - u_0||
	\]
	
	\item Show that $\{ u_n \}$ is a Cauchy sequence. For arbitrary $n, k \geq 1$, using the triangle inequality, we have
	\begin{align*}
	||u_{n+k} - u_n|| &\leq \sum_{j=0}^{k-1} || u_{n+j+1} - u_{n+j}|| \\
	&\leq \sum_{j=0}^{k-1} L^{n+j} ||u_1 - u_0|| \\
	&= ||u_1 - u_0|| L^n \sum_{j=0}^{k-1} L^j \\
	&\leq ||u_1 - u_0|| L^n \sum_{j=0}^{\infty} L^j \\
	&= ||u_1 - u_0|| \frac{L^n}{1 - L} \\
	&\rightarrow 0 \text{ as }n \rightarrow \infty
	\end{align*}
	where, since $0 < L < 1$, we used the sum of the infinite geometric series.
	
	\item Show $\{ u_n \}$ converges to an element of $D$. Since $\{ u_n \}$ is a Cauchy sequence and $X$ is complete, $u_n$ converges to some element $u^*$ in $X$. Since $D$ is closed and $\{u_n\} \subset D$, $u^* \in D$.
	
	\item Show $u^*$ is a fixed point of $F$. Since $F$ is Lipschitz, it is continuous, thus by the definition of $u_n$
	\[
	F(u^*) = F\left(\lim_{n\rightarrow \infty} u_n\right) = \lim_{n \rightarrow \infty} F(u_n) 
	= \lim_{n \rightarrow \infty} u_{n+1} = u^*
	\]

	\item Show $u^*$ is the unique fixed point for $F$ in $D$. We do the usual thing, which is to suppose there are two and take the difference. If $\tilde{u}^*$ is another fixed point, then
	\begin{align*}
	|u^* - \tilde{u}^*| = |F(u^*) - F(\tilde{u}^*)| < L|u^* - \tilde{u}^*|,
	\end{align*}
	which is impossible since $L < 1$.
\end{enumerate}
\end{proof}
\end{theorem}

It is important to note that to use the Banach fixed point theorem, we require a closed subspace $D$, and that we have to verify two things:
\begin{enumerate}
    \item $F: D \rightarrow D$
    \item $F$ is a contraction on $D$
\end{enumerate}
It turns out that often the first is the hardest one to verify, and that once that is done, we often get the second for free.

Since we discussed Newton's method above, let's revisit it using the Banach fixed point theorem. We prove the following lemma, which gives us a criterion for when Newton's method converges.

\begin{lemma}[Convergence of Newton's Method]
Let $f: [a, b] \rightarrow \R$ be $C^2$. Suppose for some $x \in [a, b]$ that $f(x) = 0$ and $f'(x) \neq 0$. Then there exists an interval $I = [x - \delta, x + \delta] \subset [a, b]$ such that Newton's method converges to $x$ starting at any $x_0 \in I$.
\begin{proof}
Define the ``Newton function''
\[
g(x) = x - \frac{f(x)}{f'(x)}
\]
We will show that there is an interval containing $x$ on which $g$ is a contraction. For some $\delta > 0$ to be chosen later, let $I = [x - \delta, x + \delta ] \subset [a, b]$. Then for any $y_1, y_2 \in I$, since $g$ is continuously differentiable, it follows from the mean value theorem that
\begin{align*}
|g(y_1) - g(y_2)| \leq \sup_{y \in I} |g'(y)| \: |y_1 - y_2|
\end{align*}
All we have to do is choose $\delta$ sufficiently small to control $g'(y)$. Evaluating $g(y)$, we have
\[
g'(y) = 1 - \frac{f'(y)^2 - f(y)f''(y)}{f'(y)^2} = \frac{f(y)f''(y)}{f'(y)^2}
\]

Now comes the technical part. Since $f$, $f'$, and $f''$ are continuous, $f(x) = 0$, and $f'(x) \neq 0$, choose $\delta$ sufficiently small so that for all $y \in I$,
\begin{enumerate}[(i)]
	\item $|f'(y)| \geq \frac{1}{2}|f'(x)|$ 
	\item $|f''(y)| \leq \frac{3}{2}|f''(x)|$
	\item $|f(y)| \leq \frac{1}{6} \frac{|f'(x)|}{ |f''(x)}$
\end{enumerate}
Then for all $y \in I$
\begin{align*}
|g(y_1) - g(y_2)| &\leq 
\left( \frac{|f'(x)|}{6 |f''(x)} \frac{3}{2}|f''(x)| \frac{2}{|f'(x)|}\right)|y_1 - y_2| \\ 
&\leq \frac{1}{2}|y_1 - y_2| 
\end{align*}
Since the Newton map $g$ is a contraction on $I$, by the Banach Fixed Point Theorem it has a unique fixed point $x^*$ in $I$. Since $x$ is also a fixed point of $g$ in $I$, by uniqueness we have $x^* = x$. Since Newton's method uses the technique of successive approximations, which is what was done in the proof of the Banach Fixed Point Theorem, Newton's method must converge to $x$.
\end{proof}
\end{lemma}

Another consequence of the Banach fixed point theorem is the inverse function theorem. For motivation, we will look at the one-dimensional case. Suppose $f: \R \rightarrow \R$ is continuously differentiable at $x = a$. We know from calculus that if $f'(a) \neq 0$, $f$ is invertible in a neighborhood of $a$. This makes sense, since if $f'$ is continuous and $f'(a) \neq 0$, $f$ is either strictly increasing or strictly decreasing in a neighborhood of $a$. We can then read the inverse off of the graph of $f$ near $a$. Using the chain rule on $f^{-1}(f(x)) = x$ at $x = a$, if $b = f(a)$, then
\[
(f^{-1})'(b) = \frac{1}{f'(a)}
\]

The inverse function theorem extends this to higher dimensions. We will prove this later as a corollary of the Implicit Function Theorem (this is easier to do!), but we will state the theorem here and outline how the proof uses the Banach fixed point theorem.

\begin{theorem}[Inverse Function Theorem]
Let $F: \R^n \rightarrow \R^n$ be continuously differentiable, and suppose the derivative $DF(x_0)$ is invertible at $x_0$. Then $F$ is invertible in a neighborhood of $x_0$. Precisely,

\begin{enumerate}
\item There exist neighborhoods $U$ of $x_0$ and $V$ of $y_0 = F(x_0)$, such that the restriction $F|_U: U \rightarrow V$ is a bijection.
\item The inverse $G: V \rightarrow U$ is also continuously differentiable, and for $y \in V$,
\begin{align}
DG(y) &= DF(G(y))^{-1}
\end{align}
\end{enumerate}
\begin{proof}
For convenience, take $x_0 = 0$. An outline of the proof is as follows.
\begin{enumerate}
    \item Since $DF(0)^{-1} DF(x)$ is continuous at $x = 0$ and $DF(0)^{-1} DF(0) = I$, we can find $\delta > 0$ such that for all $||x|| \leq \delta$,
    \begin{enumerate}
    \item $||I - DF(0)^{-1} DF(x)|| \leq \frac{1}{2}$
    \item $DF(x)$ is nonsingular
    \end{enumerate}
    Let $B = \overline{ B_\delta(0) }$ (closed ball of radius $\delta$ about 0).

    \item For a fixed $y$ (which we take as a parameter), define the ``Newton map'' 
    \begin{equation*}
    N(x; y) = x - DF(0)^{-1}(F(x) - y)
    \end{equation*}
    Note the resemblance of this map to that used in Newton's method. Since $DF(0)^{-1}$ is nonsingular, $x$ is a fixed point of $N(\cdot; y)$ if and only if $y = F(x)$.

    \item Verify the hypotheses of the Banach fixed point theorem
    \begin{enumerate}
    \item Show $N(\cdot; y)$ is contraction on $B$.
    \item Show $N(\cdot; y): B \rightarrow B$ for all $y$ in a neighborhood $V$ of $f(0)$.
    \end{enumerate}
    
    \item Use the Banach fixed point for all $y \in V$ to find a unique $x \in B$ such that $f(x) = y$. Let $f^{-1}(y)$ be this unique $x$.
    
    \item Show $f^{-1}(y)$ is continuous and differentiable on $V$. This is annoying, but it can be done. Once we have proved the Uniform Contraction Mapping Principle, this will be much easier to do.
\end{enumerate}
\end{proof}
\end{theorem}

The next theorem we will prove is the Uniform Contraction Mapping Principle. The idea here is that we have a family of contraction maps $F(x; \mu)$ indexed by a parameter $\mu$. For each value of the parameter $\mu$, $F(\cdot; \mu)$ has a unique fixed point by the Banach Fixed Point Theorem. Let $G(\mu)$ map each $\mu$ to that unique fixed point. The Uniform Contraction Mapping Principle says that the map $G$ is as smooth as the original map $F$. The proof incorporates elements from Humpherys, Jarvis, and Evans, \emph{Foundations of Applied Mathematics, Volume 1: Mathematical Analysis} (2017). We will first need one technical result involving operator norm bounds on derivatives of Lipschitz functions. 

\begin{proposition}
Let $X$, $Y$ Banach spaces, $U$ an open subset of $X$, and $F: U \subset X \rightarrow Y$ differentiable. If $F$ is Lipschitz with constant $L$, then for all $x \in U$ the operator norm of $DF(x)$ has bound $||DF(x)|| \leq L$.
\begin{proof}
Let $\epsilon > 0$ and $x \in U$. Since $F$ is differentiable at $x$, we can find $\delta > 0$ such that whenever $||h||_X < \delta$,
\[
\frac{||F(x+h) - F(x) - DF(x)h||_Y}{||h||_X} < \epsilon
\]
We use the ``supremum of unit vectors'' definition of the operator norm (definiton 2 above). Let $u \in U$ be a unit vector, and let
\[
h = \frac{\delta}{2}u
\]
Then since $u = h/||h||$, we have from the triangle inequality
\begin{align*}
||DF(x)u|| &= \frac{||DF(x)h||}{||h||_X} \\
&\leq \frac{||DF(x)h - (F(x+h) - F(x))||_Y + ||F(x+h) - F(x)||_Y }{||h||_X} \\
&= \frac{||F(x+h) - F(x) - DF(x)h||_Y }{||h||_X} + \frac{||F(x+h) - F(x)||_Y }{||h||_X} \\
&\leq \epsilon + \frac{L||x + h - x||_X}{||h||_X} \\
&= \epsilon + \frac{L||h||_X}{||h||_X} \\
&= \epsilon + L
\end{align*}
Since $\epsilon$ is arbitrary and this is independent of $u$, we conclude that $||DF(x)|| \leq L$ for all $x \in U$.
\end{proof}
\end{proposition}

\begin{theorem}[Uniform Contraction Mapping Principle]
Let 
\begin{enumerate}
\item $(X, |\cdot|)$ be a Banach space
\item $D \subset X$ a closed, nonempty subset of $X$
\item $B$ and open subset of Banach space $Y$ (the ``parameter space'').
\item $F: D \times B \rightarrow D$ a map which is \emph{uniform contraction}, i.e. there exists a constant $L < 1$ such that for all $\mu \in B$ and $u, v \in D$
\begin{equation}
|F(u, \mu) - F(v, \mu)| \leq L|u - v|
\end{equation}
\end{enumerate}

Let $G: B \rightarrow D$ be the map which associates every $\mu \in B$ with the unique fixed point of $F(\cdot; \mu)$ from the Banach Fixed Point Theorem. Then

\begin{enumerate}[(i)]
\item If $F$ is uniformly Lipschitz in $\mu$, i.e. if there exists a constant $M > 0$ such that for all $u \in D$ and $\mu_1, \mu_2 \in B$, 
\[
|F(u, \mu_1) - F(u, \mu_2)| \leq M |\mu_1 - \mu_2|
\]
then $G$ is Lipschitz, with Lipschitz constant $M / (1 - L)$.
\item If $F \in C^k(D \times B, X)$ for $k \geq 0$, then $G \in C^k(B, X)$. $k = \infty$ is allowed.
\end{enumerate}
\begin{proof}

\begin{enumerate}
Define $G(\mu)$ as in the statement of the theorem. By the Banach Fixed Point Theorem, $G: D \rightarrow B$ is the unique function so that $F(x; \mu) = x$ if and only if $x = G(\mu)$. We proceed in the following steps.

\item Since we would like to show continuity of $G$, we first derive an expression for $|G(\mu_1) - G(\mu_2)|$. Using the fact that $G(\mu)$ is a fixed point of $F(\cdot; \mu)$ and the triangle inequality
\begin{align*}
|G(\mu_1) - G(\mu_2)| &= |F( G(\mu_1); \mu_1) - F(G(\mu_2); \mu_2)| \\
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)| + |F( G(\mu_1); \mu_2) - F(G(\mu_2); \mu_2)| \\
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)| + L |G(\mu_1) - G(\mu_2)|
\end{align*}
Since $0 < L < 1$, subtract the last term on the RHS from both sides to get
\begin{align*}
(1 - L)|G(\mu_1) - G(\mu_2)| 
&\leq |F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)|
\end{align*}
Divide by $(1 - L)$ to get 
\begin{align}\label{Gmudiff}
|G(\mu_1) - G(\mu_2)| &\leq \frac{1}{1-L}|F( G(\mu_1); \mu_1) - F(G(\mu_1); \mu_2)|
\end{align}

\item If $F$ is continuous in both variables (which is case (ii) with $k = 0$, then the RHS of \eqref{Gmudiff} $\rightarrow 0$ as $\mu_2 \rightarrow \mu_1$, thus $G$ is continuous.

\item For case (i), if $F$ is uniformly Lipschitz in $\mu$, then \eqref{Gmudiff} becomes 
\begin{align*}
|G(\mu_1) - G(\mu_2)| &\leq \frac{M}{1-L} |\mu_1 - \mu_2| \\
\end{align*}

\item All that remains is consider case (ii) with $k > 0$. We first consider the case $k = 1$, i.e. $F$ is continuously differentiable. This turns out to be a mess to show. The way we will do this is to devise a candidate for $DG(\mu)$ and then use the definition of the derivative to show that this candidate is indeed the derivative.

\item Recall that $G(\mu) = F(G(\mu),\mu)$. If $G$ were differentiable, then by the chain rule and differentiability of $F$ we would have
\[
DG(\mu) = DF(G(\mu),\mu) = D_X F(G(\mu), \mu)DG(\mu) + D_B F(G(\mu), \mu)
\]
This means that $DG(\mu)$ is a fixed point of the mapping $\Phi: \mathcal{L}(B, X) \times B \rightarrow \mathcal{L}(B, X)$ defined by
\[
\Phi(A; \mu) = D_X F(G(\mu), \mu) A + D_B F(G(\mu), \mu)
\]
The map $\Phi$ is a uniform contraction, since for $A_1, A_2 \in \mathcal{L}(B, X)$
\begin{align*}
|\Phi(&A_1; \mu) - \Phi(A_2; \mu)| \\
&= | D_X F(G(\mu), \mu) A_1 + D_B F(G(\mu), \mu) - (D_X F(G(\mu), \mu) A_2 + D_B F(G(\mu), \mu)) | \\
&= | D_X F(G(\mu), \mu) (A_1 - A_2) | \\
&\leq ||D_X F(G(\mu), \mu) ||\:|A_1 - A_2| \\
&\leq L |A_1 - A_2|
\end{align*}
where the last line follows from the previous proposition and the fact that $F(\cdot; \mu)$ is Lipschitz with constant $L$.

Since $L < 1$, by the Uniform Contraction Mapping Principle (the part we have proved so far!), there exists a function $Z: B \rightarrow \mathcal{L}(B, X)$ which maps each $\mu \in B$ to the unique fixed point $Z(\mu)$ of $\Phi(\cdot; \mu)$. Since $F$ is $C^1$, $\Phi$ is continuous, thus by the $k = 0$ case (already proved!), the map $Z(\mu)$ is continuous.
    
\item We have a candidate $Z(\mu)$ for $DG(\mu)$. All that remains is to use the definition of the derivative to show that $Z(\mu)$ is actually the derivative. This is very technical and time-consuming, so we will omit it. If you are interested in how this is done, you can look the proof of Lemma 7.2.9 on pages 284-285 of Humpherys, Jarvis, and Evans (2017).

\item We repeat this argument for $k > 1$.
\end{enumerate}

\end{proof}
\end{theorem}

We will use the uniform contraction mapping principle to prove the implicit function theorem, which is one of the most important tools in analysis dynamical systems. We will start with an example.

Consider the unit circle, $x^2 + y^2 = 1$. This is not a function (of $y$ in terms of $x$), since other than the right and left endpoints $(0, 1)$ and $(0, -1)$, there are two values of $y$ for every $x$. That being said, at any point other than $(0, 1)$ and $(0, -1)$, we can solve \emph{locally} for $y$ as a function of $x$. To do this, we draw a small box around that point, ignore everything outside of the box, and what is left is $y$ as a function of $x$. In this case, we can obtain an explicit formula for the function in the box. (We will usually not be able to do this!) Near any point on the upper semicircle, for example, we have $y = \sqrt{1 - x^2}$.

Why does this fail at the left and right endpoints? Let's rewrite the unit circle as the zero set of the function $f(x, y) = x^2 + y^2 - 1$. At $(0, 1)$, we have $\partial f / \partial y = 0$ and $\partial f / \partial y$. In other words, near $(0, 1)$, $f$ resembles a vertical line.

We can now state and prove the implicit function theorem. The proof uses the Uniform Contraction Mapping Principle.

\begin{theorem}[Implicit Function Theorem]
Let
\begin{enumerate}
\item $X$, $Y$, $Z$ Banach spaces
\item $U \subset X$, $V \subset Y$ open sets
\item $F: U \times V \rightarrow Z$ a $C^k$ map, $k \geq 1$
\item $(x_0, y_0) \in U \times V$ with $F(x_0, y_0) = 0$
\item The partial derivative $D_X F(x_0, y_0): X \rightarrow Z$ is invertible with bounded inverse
\end{enumerate}
Then we can solve for $x$ as a function of $y$ near $(x_0, y_0)$. That is, there is a neighborhood $U_0 \times V_0 \subset U \times V$ of $(x_0, y_0)$ and a unique $C^k$ function $f: V_0 \rightarrow U_0$ with $f(y_0) = x_0$ such that $F(x, y) = 0$ for $(x, y) \in U_0 \times V_0$ if and only if $x = f(y)$. Furthermore, the derivative of $f$ satisfies
\[
Df(y) = -D_X F(f(y),y)^{-1} D_Y F(f(y),y)
\]

\begin{proof}
\begin{enumerate}
\item First, we define a ``Newton map''. Since $D_X F(x_0, y_0)$ is invertible, define the map $G: U \times V \rightarrow X$ by 
\begin{equation}\label{defG}
G(x, y) = x - [D_X F(x_0, y_0)]^{-1} F(x, y)
\end{equation}
This looks a lot like the map use use for Newton's method, except the derivative is always evaluated at $(x_0, y_0)$. For fixed $y \in V$, $G(x, y) = x$ if and only if $F(x, y) = 0$. Since $F$ is $C^k$, $G$ is $C^k$ as well, and
\[
DG_X(x, y) = I - [D_X(x_0, y_0)]^{-1} \: DF_X(x, y)
\]
In particular, $DG_X(x_0, y_0) = 0$.

\item We will show that $G$ is a uniform contraction on a neighborhood of $(x_0, y_0)$. We do this in several steps.

\item First, we control $DG_X(x, y)$ near $(x_0, y_0)$. Since $DG_X(x, y)$ is continuous and $DG_X(x_0, y_0) = 0$, we can find open balls $U_0 = B_\delta(x_0)$ and $V_0 = B_\epsilon(y_0)$ such that for $(x, y) \in \overline{U_0} \times V_0$,
\[
||DG_X(x, y)|| < \frac{1}{2} 
\]
Since 
\[
[D_X(x_0, y_0)]^{-1} DF_X(x, y) = I - DG_X(x, y)
\]
and $||DG_X(x, y)|| < \frac{1}{2}$ on $\overline{U_0} \times V_0$, it follows from the Neumann series that $DF_X(x, y)$ is invertible for $(x, y) \in \overline{U_0} \times V_0$.

\item Next, we control $F(x_0, y)$ near $(x_0, y_0)$. For convenience, let $L = [D_X(x_0, y_0)]^{-1}$. Since  the map $y \mapsto F(x_0, y)$ is continuous in $y$ and $F(x_0, y_0) = 0$, we can (if needed) decrease $\epsilon$ so that for $y \in V_0$
\[
|F(x_0, y)| \leq \frac{\delta}{2 ||L||}
\]

\item Next, we show that $G: \overline{U_0} \times V_0 \rightarrow \overline{U_0}$. To do this we will use the following result. If $f: X \rightarrow Y$ is continuously differentiable map between Banach spaces, $U \subset X$ is convex, and $\sup_{u \in U} ||Df(u)|| \leq M$, then $f$ is Lipschitz on $U$ with constant $M$. We proved this result for subsets of $\R^n$; the proof for the more general case is identical.

Using this and the triangle inequality, for $x \in \overline{U_0}$ and $y \in V_0$,
\begin{align*}
|G(x, y) - x_0| &\leq |G(x, y) - G(x_0, y)| + |G(x_0, y) - x_0| \\
&\leq \sup_{x\in \overline{U_0}}||DG_x(x, y)||\:|x - x_0| + |x_0 - L F(x_0, y) - x_0 | \\
&< \frac{1}{2} |x - x_0| + |L F(x_0, y)| \\
&\leq \frac{\delta}{2} + ||L|| \frac{\delta}{2 ||L||} \\
&\leq \delta
\end{align*} 
thus we conclude
\[
|G(x, y) - x_0| < \delta
\]
for all $x \in \overline{U_0}$ and $y \in V_0$.

\item Finally, we show that $G$ is a uniform contraction. For $x_1, x_2 \in \overline{U_0}$ and any $y \in V_0$,
\[
|G(x_1, y) - G(x_2, y)| \leq \sup_{x\in \overline{U_0}}||DG_x(x, y)||\:|x_1 - x_2| \leq \frac{1}{2}|x_1 - x_2|
\]

\item By the Uniform Contraction Mapping Principle, there is a unique $C^k$ function $f: V_0 \rightarrow \overline{U_0}$ which maps $y \in V_0$ to the unique fixed point $x$ of $G(\cdot, y)$. In other words, $G(f(y),y) = f(y)$ for all $y \in V_0$. For all $y \in V_0$, since $f(y) \in \overline{U_0}$, using the result from step 5,
\[
|f(y) - x_0| = |G(f(y), y) - x_0| < \delta
\]
Thus $f(y) \in U_0$ (not just $\overline{U_0}$), and so we can restrict the codomain of $f$ to $U_0$, i.e. we have $f: V_0 \rightarrow U_0$.

\item Since $F(x, y) = 0$ if and only if $x$ is a fixed point of $G(\cdot, y)$, this implies that 
	\begin{enumerate}
	\item $f(y_0) = x_0$
	\item For $(x, y) \in U_0 \times V_0$, $F(x, y) = 0$ if and only if $x = f(y)$
	\end{enumerate}
which is the result we want!

\item To compute the derivative of $f$, use the chain rule on $F(f(y), y) = 0$.
\begin{align*}
0 &= D_X F(f(y),y)Df(y) + D_Y F(f(y),y)
\end{align*}
Since $D_X F(x, y)$ is invertible on $U_0 \times V_0$, we can rearrange this to get
\[
Df(x) = -D_X F(f(y),y)^{-1} D_Y F(f(y),y)
\]
\end{enumerate}
\end{proof}
\end{theorem}

The inverse function theorem is a corollary of the implicit function theorem.

\begin{theorem}[Inverse Function Theorem]
Let
\begin{enumerate}
\item $X$, $Y$ Banach spaces
\item $U \subset X$, $V \subset Y$ open sets
\item $G: U \rightarrow V$ a $C^k$ map with $G(x_0) = y_0$
\item The derivative $D F(x_0)$ is invertible with bounded inverse
\end{enumerate}
Then we can invert $G$ near $y_0$. That is, there exist neighborhoods $U_0 \subset U$ of $x_0$ and $V_0 \subset V$ of $y_0$ and a unique $C^k$ function $G^{-1}: V_0 \rightarrow U_0$ such that $G(G^{-1}(y)) = y$ for all $y \in V_0$ and $G^{-1}(G(x)) = x$ for all $x \in U_0$. Furthermore, the derivative of $G^{-1}$ is given by
\[
DG^{-1}(y) = D G( G^{-1}(y) )^{-1}
\]

\begin{proof}
Take $F(x, y) = G(x) - y$ and use the implicit function theorem.
\end{proof}
\end{theorem}

As a final application of the uniform contraction mapping principle, we will revisit existence and uniqueness of solutions to ODEs. We take the easier case where the ODE is written $\dot{u} = f(u)$, with $f$ independent of $t$.

\begin{theorem}[Picard-Lindel\"{o}f Existence and Uniqueness Theorem]
Consider the initial value problem on $\R^n$
\begin{align}\label{IVPp}
\frac{du}{dt} &= f(u) \\
u(0) &= u_0 \nonumber
\end{align}
Suppose that $f$ is locally Lipschitz, i.e. for every $\tilde{u} \in \R^n$ there exists a radius $\delta$ and a Lipschitz constant $L$ (both depending on $\tilde{u}$) such that for all $u_1, u_2 \in B(\tilde{u}, \delta)$,
\[
|f(u_1) - f(u_2)| \leq L|u_1 - u_2|
\]
Then for every $\tilde{u} \in \R^n$ there exists a radius $\delta$ and a time interval $[-r, r]$ such that
\begin{enumerate}[(i)]
\item For each initial condition $u_0 \in B(\tilde{u}, \delta)$ the initial value problem has a unique solution $u(t; u_0)$ on $[-r, r]$.
\item The map $u_0 \rightarrow u(\cdot; u_0)$ is Lipschitz in $u_0$.
\item If $f$ is $C^k$ for $k \geq 1$, then
\begin{enumerate}
\item The solution $u(t; u_0)$ is $C^{k+1}$ in $t$
\item The map $u_0 \rightarrow u(\cdot; u_0)$ is $C^k$ in $u_0$
\end{enumerate}
\end{enumerate}
\begin{proof}
The outline of the proof is as follows: write the IVP in integrated form, use the integrated form to construct a linear operator between Banach spaces, show that this operator is a uniform contraction, and then apply the Uniform Contraction Mapping Principle.
\begin{enumerate}

\item First, we reformulate the problem as an integral equation. By the fundamental theorem of calculus, if $u$ is differentiable, then
\begin{equation*}
u(t) = u(0) + \int_0^t u'(\tau) d \tau
\end{equation*}
Thus $u(t)$ is a solution to the initial value problem \eqref{IVPp} if an only if it is a solution to the integral equation
\begin{equation}\label{intform}
u(t) = u(0) + \int_0^t f(u(\tau)) d \tau
\end{equation}

\item Next, we define some constants. Choose any $\tilde{u} \in \R^n$. Since $f$ is locally Lipschitz, we can find a radius $\delta > 0$ and a constant $L > 0$ such that for $u_1, u_2 \in B_{2 \delta}(\tilde{u})$,
\begin{align*}
|f(u_1) - f(u_2)| \leq L |u_1 - u_2|
\end{align*}
Since $f$ is continuous, it is bounded in the closure of this ball, so let
\[
C = \sup \{ f(u) : u \in \overline{ B_{2 \delta}(\tilde{u}) } \}
\]
Finally, for convenience, let
\[
B = B_\delta(\tilde{u})
\]
be the set of initial conditions we will consider.

\item Now we define our Banach spaces. Consider the closed interval $[-r, r]$, where $r > 0$ is small and will be chosen later. We will show that we have a unique solution for $t \in [-r, r]$. Let
\begin{align*}
X = C^0([-r, r], \R^n)
\end{align*}
Equipped with the supremum norm, $X$ is a Banach space. Let $k(t) \in X$ be the constant function $\tilde{u}$, i.e.
\begin{align*}
k(t) &= \tilde{u} \text{ for all } t \in [-r, r]
\end{align*}
Finally, let $D$ be the closed ball in $X$ of radius $2 \delta$ about the constant function $k(t)$, i.e.
\begin{align*}
D = \overline{ B_{2\delta}(k(t)) } = \{ u \in X : \sup_{t \in [-r,r]} | u(t) - \tilde{u} | \leq 2 \delta \}
\end{align*}
\item Next, we define our mapping between Banach spaces. Define $F: D \times B \rightarrow X$ by
\begin{align}\label{defF}
[F(u, u_0)](t) = u_0 + \int_0^t f(u(\tau)) d \tau
\end{align}
which is the RHS of the integrated form of the initial value problem. As we noted above, a fixed point of $F$ is a solution to the the IVP. Since $f$ is at least continuous, the RHS of \eqref{defF} is in $X$.

\item Show that for sufficiently small $r$, $F: D \times B \rightarrow D$, i.e. the RHS of \eqref{defF} is actually in $D$. For $u \in D$, $u_0 \in B$,
\begin{align*}
\sup_{|t| \leq r} |F(u, u_0) - \tilde{u}| &= 
\sup_{|t| \leq r} \left|u_0 + \int_0^t f(u(\tau)) d\tau - \tilde{u}\right| \\
&\leq |u_0 - \tilde{u}| + \int_0^r |f(u(\tau))| d \tau \\
\end{align*}
Since we are taking $u \in D$, $u(\tau) \in B_{2 \delta}(\tilde{u})$ for all $\tau \in [-r, r]$, and so $|f(u(\tau)| \leq C$ for all $\tau \in [-r, r]$. Thus we have
\begin{align*}
\sup_{|t| \leq r} |F(u, u_0) - \tilde{u}|
&< \delta + C r \\
\end{align*}
If we take $r \leq \delta/C$, we have $\sup_{|t| \leq r} |F(u, u_0) - \tilde{u}| < 2 \delta$, which is what we want.

\item Show that for sufficiently small $r$, $F$ is a contraction. For $u, v \in D$ and $u_0 \in B$, 
\begin{align*}
\sup_{|t|\leq r}&|F(u, u_0) - F(v, u_0)| \\
&=\sup_{|t|\leq r} \left| u_0 + \int_0^t f(u(\tau)) d \tau - \left(u_0 + \int_0^t f(v(\tau)) d \tau \right) \right| \\
&\leq \sup_{|t|\leq r} \int_0^t | f(u(\tau)) - f(v(\tau))| d \tau \\
&\leq L r \sup_{|t|\leq r} |u(t) - v(t)| \\
&\leq L r |u - v|
\end{align*}
If we take $r \leq 1/2L$, $F$ is a contraction. Thus if we choose $r = \min\{ \delta/C, 1/L\}$, we are all set!

\item Show that the map $F$ is uniformly Lipshitz in the initial condition $u_0$. For $u_0, u_1 \in B$,
\begin{align*}
|F(u, u_0) - F(u, u_1)| = |u_0 - u_1| \leq |u_0 - u_1|
\end{align*} 
Thus $F$ is uniformly Lipschitz in $u_0$ with Lipschitz constant 1.

\item Finally, we show that $F$ is $C^k$ (in both variables) whenever $f$ is $C^k$. Since $F$ is linear in $u_0$, $F$ is smooth in $u_0$, thus it remains to show that $F(\cdot, u_0)$ is $C^k$. We will show that $F(\cdot, u_0)$ is $C^1$ if $f$ is $C^1$. Let $u(t) \in D$. We make an educated guess that the Fr\'{e}chet (partial) derivative with respect to $u$ at $u(t)$ is 
\[
L h(t) = \int_0^t Df(u(\tau)) h(\tau) d \tau
\]
Where did we get this from? One way to see it is by Taylor expanding $F(u(t) + h(t), u_0)$ and taking the term which is linear in $h(t)$. 

By the properties of integration, $L$ is a linear operator, and it is bounded since $f$ is $C^1$. Using the definition of the Fr\'{e}chet derivative,
\begin{align*}
\frac{||F(u + h) - F(u) - L(h) ||}{||h||} 
&\leq \frac{1}{||h||}
\sup_{|t| \leq r} \left| \int_0^t [ f(u(\tau) + h(\tau)) - f(u(\tau))] d\tau - \int_0^t Df(u(\tau)) h(\tau) d \tau \right| \\
&= \frac{1}{||h||}
\sup_{|t| \leq r} \left| \int_0^t [ f(u(\tau) + h(\tau)) - f(u(\tau)) - Df(u(\tau))h(\tau)] d\tau\right| \\
&\leq \int_0^r \frac{ | f(u(\tau) + h(\tau)) - f(u(\tau)) - Df(u(\tau))h(\tau)|}{|h(\tau)|} d\tau
\end{align*}
as $||h|| \rightarrow 0$, $|h(\tau)| \rightarrow 0$, thus the RHS of this goes to 0 since $f$ is continuously differentiable and we are on a closed interval $[0, r]$. We can repeat this to conclude that $F$ is $C^k$.

\item We have now satisfied the hypotheses of the Uniform Contraction Mapping Principle. Thus there exists a $C^k$ map $G: B \rightarrow D$ which associates maps each initial condition $u_0$ to the unique fixed point of $F(\cdot; u_0)$, which is the unique solution $u(t; u_0)$ on $[-r, r]$ to \eqref{IVPp}. Note that this is a local existence result, but the interval of existence $[-r, r]$ is the same for all initial conditions $u_0 \in B$.

\item Finally, the solution $u(t; u_0)$ is $C^{k+1}$ whenever $f$ is $C^k$. Since we now know that $u(t)$ is a solution to the IVP, this follows from repeatedly differentiating $\frac{du(t)}{dt} = f(u(t))$.

\end{enumerate}
\end{proof}
\end{theorem}

Finally, we look at what happens when a solution approaches the boundary of the region where the solution exists. The following theorem shows that such a solution blows up at the boundary.

\begin{theorem}[Blowup at Boundary]
Suppose $f: \R^n \rightarrow \R^n$ is $C^1$, and $u(t)$ satisfies the IVP
\begin{align*}
\dot{u} = f(u) \\
u(0) = u_0 
\end{align*}
on an interval $[0, T)$, but there is no solution to the IVP on the interval $[0, T + \epsilon)$ for any $\epsilon > 0$. Then $u$ blows up as it approaches the boundary, i.e. $|u(t)| \rightarrow \infty$ as $t \rightarrow T$.

\begin{proof}
We employ a proof by contradiction.
\begin{enumerate}
\item Suppose this is not true. Then there is some sequence $t_n$ with $t_n \uparrow T$ such that $\{ u(t_n) \}$ remains bounded, i.e. there exists a constant $K$ such that $|u(t_n)| \leq K$ for all $n$.

\item Since $\{ u(t_n) \}$ is a bounded sequence in $\R^n$, by Bolzano-Weierstrass it has convengent subsequence. Passing to this subsequence if needed, we may assume that $u(t_n) \rightarrow \tilde{u}$.

\item Now consider the same initial value problem, but initial condition $\tilde{u}_0$ close to $\tilde{u}$. By the existence and uniqueness theorem, there is a radius $\delta$ and an interval $[-r, r]$ such that for all initial conditions $\tilde{u}_0 \in B_\delta(\tilde{u})$, there is a unique solution $u(t)$ for $t \in [-r, r]$ with $u(0) = \tilde{u}_0$. 

\item Since $f$ does not depend on $t$, for each $\tilde{u}_0 \in B_\delta(\tilde{u})$ there is a unique solution $u(t; \tau)$ on the interval $[\tau - r, \tau + r]$ with $u(\tau; \tau) = \tilde{u}_0$. These solutions are just translates of each other.

\item Since $t_n \uparrow T$ and $u(t_n) \rightarrow \tilde{u}$, choose integer $m$ sufficiently large so that $t_m > T - r/2$ and
$u(t_m) \in B_\delta(\tilde{u})$. Consider the initial value problem. Let $\tau = t_m$ and $u_* = u(t_m)$
\begin{align*}
\dot{v} = f(v) \\
v(\tau) = u^*
\end{align*}
By what we showed above, we have a unique solution $v(t)$ for $t = [\tau - r, \tau + r]$ to this IVP with $v(\tau) = u^*$.

\item By uniqueness (since $f$ is Lipshitz on $[0, T + r/2]$, we can splice these solutions together, since $u(t)$ stops at $(\tau, u^*)$ and $v(t)$ starts at $(\tau, u^*)$. Thus we have the following solution to the original IVP.
\[
w(t) = \begin{cases}
u(t) & T \in [0, \tau] \\
v(t) & T \in [\tau, \tau + r] \subset [\tau, T + r/2]
\end{cases}
\]
Since this solution exists on a larger interval than $[0, T)$, we have a contradiction. Thus our original assumption must be false, i.e. the solution $u(t)$ must blow up as it hits the boundary.
\end{enumerate}
\end{proof}
\end{theorem}

If we have a linear system, we can obtain a global existence result. 
\begin{theorem}[Global Existence for Linear Systems]
Consider the system
\begin{align*}
\dot{u} &= A(t) u \\
u(0) &= u_0
\end{align*}
where $u \in \R^n$ and $A:\R \rightarrow \R^{n \times n}$ is continuous. Then there exists a unique solution $u(t)$ which exists for all $t \in \R$.
\begin{proof}
\begin{enumerate}
\item Write the ODE as $\dot{u} = f(u, t)$ with $f(u,t) = A(t) u$. Since $f(u, t)$ is Lipschitz in $u$ on every bounded interval $[-T, T]$, the initial value problem has a unique solution $u(t)$ for $t$ in an interval containing $0$. 
\item We wish to show that $u(t)$ exists for all $t \in \R$. Suppose this solution exists on $[0, t_0)$, but does not exist for any larger interval $[0, t_0 + \epsilon)$. Then $u(t)$ must blow up as it approaches $t_0$. We will show this cannot happen.
\item Write the ODE in integrated form as
\begin{align*}
u(t) &= u_0 + \int_0^t A(\tau)u(\tau)d\tau && t \in [s, t_0)
\end{align*}
Taking absolute values and using the matrix/operator norm of $A(t)$, this becomes
\begin{align*}
|u(t)| &= |u_0| + \int_0^t ||A(\tau)||\:|u(\tau)|d\tau && t \in [0, t_0)
\end{align*}
\item This satisfies the hypotheses of Gronwall's Inequality. Thus, for $t \in [0, t_0)$
\begin{align*}
|u(t)| &\leq |u_0| \exp \left( \int_s^t ||A(\tau)|| d\tau \right) \\
&\leq |u_0| \exp \left( \int_s^{t_0} ||A(\tau)|| d\tau \right) \\
&\leq C |u_0|
\end{align*}
where the bound holds uniformly for $t \in [0, t_0)$ since it does not depend on $t$.
\item We conclude that $u(t)$ cannot blow up as $t \rightarrow t_0$, thus $u(t)$ must exist for all $t \geq 0$. A similar argument shows that $u(t)$ must exist for all $t \leq 0$.
\end{enumerate}
\end{proof}
\end{theorem}

\section{Integration}

\subsection{Riemann Integral}

We start with a discussion of the Riemann integral, in particular its strengths and weaknesses. Let $f: [a, b] \rightarrow \R$ be a bounded function. (It is important that the domain be a closed, bounded interval and the function is bounded). The Riemann integral is defined as follows.
\begin{enumerate}
    \item Partition the domain $[a,b]$. Let $P$ be the partition
    \[
    P: a = x_0 < x_1 < \dots < x_{n-1} < x_n = b
    \]
    \item Define the upper and lower sums on $P$ by
    \begin{align*}
        U_P(f) = \sum_{j = 0}^{n-1} (x_{j+1} - x_j) \sup_{x \in [x_j, x_{j+1}]} f(x) \\
        L_P(f) = \sum_{j = 0}^{n-1} (x_{j+1} - x_j) \inf_{x \in [x_j, x_{j+1}]} f(x) \\
    \end{align*}
    \item Then $f$ is Riemann integrable if
    \[
    \inf_P U_P(f) = \sup_P L_P(f)
    \]
    If this holds, we denote the Riemann integral by $\int_a^b f(x) dx$.
    \item A convenient integrability criterion is the following: $f$ is Riemann integrable on $[a, b]$ if for every $\epsilon > 0$ we can find a partition $P$ such that
    \[
    U_P(f) - L_P(f) < \epsilon
    \]
\end{enumerate}
Technically, what we just defined is the Riemann-Darboux integral, which is equivalent to and easier to work with than the Riemann integral. The standard Riemann integral involves the Riemann sums you learned in calculus. We can define that as follows.
\begin{enumerate}
    \item Choose a tagged partition $(P, t)$ of $[a, b]$, i.e. a partition $P$ with
    \[
    P: a = x_0 < x_1 < \dots < x_{n-1} < x_n = b
    \]
    and $n$ points $\{ t_0, t_1, \dots, t_{n-1} \}$ with $t_j \in [x_j, x_{j+1}]$. The \emph{mesh size} of the partition is the maximum length of the partition intervals, i.e.
    \[
    \Delta P = \max_{j = 0, \dots, n-1}(x_{j+1} - x_j)
    \]
    \item The Riemann sum corresponding to $(P, t)$ is    
    \begin{align*}
        R_{P,t}(f) = \sum_{j = 0}^{n-1} (x_{j+1} - x_j) t_j
    \end{align*}
    \item $f$ is Riemann integrable with integral $S$ if for all $\epsilon > 0$ there exists $\delta > 0$ such that
    \[
    |R_{P, t}(f) - S| < \epsilon
    \]
    for all tagged partitions $(P, t)$ with mesh size $\Delta P < \delta$. 
\end{enumerate}
The only advantage of the standard Riemann integral is that you can compute the Riemann sums numerically. Common choices for the tags $t_i$ are the left endpoint, right endpoint, and midpoints of the partition intervals. In general, each partition interval is the same size, although this need not be the case.

The main advantages of the Riemann integral are:
\begin{itemize}
    \item You can compute them exactly with the fundamental theorem of calculus (as long as you can find an antiderivative!)
    \item The definition is intuitive, and captures the idea of finding the area under a curve by successive approximation.
    \item Many useful classes of functions are Riemann integrable.
    \begin{enumerate}
        \item Continuous functions on $[a, b]$.
        \item Bounded functions on $[a, b]$ which are continuous except at a finite number of points.
        \item Bounded, monotonic functions on $[a,b]$.
    \end{enumerate}
\end{itemize}

There are, however, several disadvantages to the Riemann integral.

\begin{itemize}
    \item It is difficult to extend to domains that are not open subsets of $\R^n$
    \item It is difficult to extend to unbounded domains such as $\R$. As in calculus class, you can define an ``improper integral'' as the limit of integrals on bounded intervals, although the best way to do this is not always clear.
    \item It can be difficult to figure out which functions are and are not Riemann integrable. As an example, write the rational numbers in $[0, 1]$ as the sequence $\{ r_k \}$, i.e.
    \[
    \Q \cap [0, 1] = \{ r_k \}_{k \in \N}
    \]
    Consider the following two examples.
    \begin{enumerate}
        \item 
        \[
        f(x) = \sum_{k=1}^\infty \frac{1}{k^2}H(x - r_k)
        \]
        where $H$ is the Heaviside step function
        \[
        H = \begin{cases}
        0 & x < 0 \\
        1 & x \geq 0
        \end{cases}
        \]
        $f(x)$ is well-defined since the sum $\sum_{k=1}^\infty 1/k^2$ is convergent. Imagine keeping a running total as you take a walk from 0 to 1 on the number line; you start a 0, and every time you pass a rational number $r_k$ you add $1/k^2$ to the total. Although $f(x)$ is discontinuous at every rational number, $f(x)$ is Riemann integrable on $[0, 1]$ since it is bounded and monotonic (increasing).
        \item Let $\chi_\Q(x)$ be the characteristic function of the set of rational numbers on $[0, 1]$, i.e.
        \[
        \chi_\Q(x) = 
        \begin{cases}
        1 & x \in \Q \\
        0 & x \notin \Q
        \end{cases}
        \]
        $\chi_\Q(x)$ is also bounded and discontinuous at every rational numbers. However, since every partition interval contains both rational and irrational numbers, $\chi_\Q(x)$ is not Riemann integrable on $[0, 1]$.
    \end{enumerate}
    
    \item The limit of a sequence of Riemann integrable functions is not necessarily Riemann integrable. Once again, let $\Q \cap [0, 1] = \{ r_k \}_{k \in \N}$, and define the sequence of functions $f_n: [0, 1] \rightarrow \R$ by
    \[
    f_n(x) = \begin{cases}
    1 & x \in {r_1, \dots, r_n} \\
    0 & \text{otherwise}
    \end{cases}
    \]
    Since $f_n$ has only a finite number of discontinuities, $f_n$ is Riemann integrable with
    \[
    \int_0^1 f_n(x) dx = 0
    \]
    For every $x \in [0, 1]$, $f_n(x) \rightarrow \chi_\Q(x)$, but $\chi_\Q(x)$ is not Riemann integrable on $[0, 1]$.
    \item It is hard to find good criteria that allow us to exchange limits and integration, i.e. conditions for which
    \[
    \lim_{n\rightarrow\infty}\int_a^b f_n(x) dx = \int_a^b \lim_{n\rightarrow\infty} f_n(x) dx
    \]
    The best we can do, in general, is if we have a uniformly convergent sequence.
    \begin{theorem}
    Let $f_n: [a, b] \rightarrow \R$ a sequence of Riemann integrable functions, and suppose $f_n$ converges uniformly to $f$. Then $f$ is Riemann integrable, and
     \[
    \lim_{n\rightarrow\infty}\int_a^b f_n(x) dx = \int_a^b f(x) dx
    \]
    \begin{proof}
    Let 
    \[
    \epsilon_n = \sup_{x \in [a, b]}|f_n(x) - f(x)|
    \]
    so that $f_n(x) - \epsilon_n \leq f(x) \leq f_n(x) + \epsilon_n$ on $[a, b]$. By uniform convergence, $\epsilon_n \rightarrow 0$. For any partition $P$ of $[a, b]$,
    \begin{align*}
    \int_a^b f_n(x) dx - (b-a)\epsilon_n &=
    \int_a^b (f_n(x) - \epsilon_n) dx \\
    &= \sup_P \sum_{j = 0}^{n-1} (x_{j+1} - x_j) \inf_{x \in [x_j, x_{j+1}]} (f(x) - \epsilon_n) \\
    &\leq \sup_P \sum_{j = 0}^{n-1} (x_{j+1} - x_j) \inf_{x \in [x_j, x_{j+1}]} f(x) \\
    &= \sup_P L_p(f)
    \end{align*}
    Similarly,
    \[
    \inf_P U_p(f) \leq \int_a^b f_n(x) dx - (b-a)\epsilon_n
    \]
    Thus we have
    \begin{align*}
    \int_a^n f_n(x) dx - (b-a)\epsilon_n \leq \sup_P L_p(f) \leq \inf_P U_p(f) \leq \int_a^b f_n(x) dx - (b-a)\epsilon_n
    \end{align*}
    which rearranges to
    \[
    0 \leq \inf_P U_P(f) - \sup_P L_P(f) \leq 2(b-a)\epsilon_n \rightarrow 0
    \]
    \end{proof}
    \end{theorem}
    Note that for this theorem to hold, it was essential that we were on a bounded interval. The problem here is that uniform convergence is a strong condition. Consider the sequence of functions $f_n(x) = x_n$ on $[0, 1]$. For the limit, $f_n(x) \rightarrow f(x)$, where
    \[
    f(x) = \begin{cases}
    0 & x \in [0, 1) \\
    1 & x = 1
    \end{cases}
    \]
    but this convergence is not uniform. (Either we can show that directly, or use the fact that if it were uniform, the limit would be continuous, which it is not in this case.) But we still have
    \[
    \lim_{n \rightarrow \infty} \int_0^1 f(x) dx = \lim_{n \rightarrow \infty} \frac{1}{1+n} = 0 = \int_0^1 f(x) dx
    \]
    Thus even though we do not have uniform convergence, the limit of the integrals is still equal to the integral of the limit. Something else must be going on, and it would be nice to have a theory which captures this case.
\end{itemize}

\subsection{Lebesgue Integral}

The Lebesgue integral is a complete different way of defining the integral. For this discussion (and for comparison purposes), we will only consider nonnegative, bounded, real-valued functions defined on a closed interval $[a, b]$, although we will soon see that the theory is more general than this. The secret sauce here is that instead of partitioning the domain, we will partition the range.

Assume $f:[a,b] \rightarrow \R$ is bounded and nonnegative, and let $M = \sup_{x \in [a,b]} f(x)$. For now, we define the Lebesgue integral as follows. (Note that this is an intuitive construction; ultimately this is not the definition we will use.)
\begin{enumerate}
\item Let $P$ be a partition the range of $f$, i.e.
\[
P : 0 = y_0 < y_1 < y_2 < \dots < y_{n-1} < y_n = M
\]
\item For $j = 0, dots, n-1$, define the set $I_j$ by
\[
I_j = f^{-1}([y_{j+1}, \infty))
\]
This set is all values of $x$ for which $f(x) \geq y_{j+1}$.
\item Define the sum
\[
I_P(f) = \sum_{j=0}^{n}(b_{j+1}-b_{j})m(I_j)
\]
where $m(I_j)$ is the ``measure'' of the set $I_j$. We have not defined this yet (this is what measure theory is all about!), but think of it as the ``length'' of $I_j$.  We can think of this as constructing layers of a cake. 
\item Then we define the Lebesgue integral as
\[
\int_{a}^{b}f(x)dx = \sup_P I_P(f)
\]
if the supremum exists.
\end{enumerate}

The Lebesgue integral resolves most of the weaknesses of Riemann integral. The main drawback is that in most cases you cannot actually compute an integral using the Lebesgue formulation and have to fall back on the fundamental theorem of calculus from Riemann integration theory. Luckily, if a function is Riemann integrable, it is also Lebesgue integrable, and the two integrals are the same! In fact, we can use Lebesgue integration theory to tell us what are the ``worst'' functions which are still Riemann integrable.

In order to use the Lebesgue integral, we need to understand the ``measure'' term $m(I_j)$ in the definition of $I_p(f)$. The field of mesaure theory is devoted to exactly this.

\section{Measure Theory}

What is measure theory, and why should we care? Consider the following two sets: $[0, 1]$ and $\Q$. There are (at least) two different notions of the ``size'' of these sets.
\begin{enumerate}
    \item ``Number of elements'' (cardinality). Set theorists would say that $[0, 1]$ is countable and $\Q$ is uncountable. 
    \item ``Length'' (measure). Intuitively, the interval $[0, 1]$ has length 1. If the empty set has 0 length and $\R$ has infinite length, then the ``length'' of $\Q$ (whatever that means) should be somewhere in the middle, although where in the middle is not clear.
\end{enumerate}

The central goal of measure theory is to generalize our intuitive ideas of length, area, and volume to subsets of arbitrary sets. This will be applied to familiar settings like the line $\R$ and the plane $\R^2$, and we will see that our new theory not only encompasses our commonsense notions of length and area but also allows us to ``measure'' subsets which we would not be able to do with a ruler.

\begin{enumerate}
	\item Probability. Take any sample spaces you want, and declare that the measure of the entire sample space is 1. Then the measure of any subset of the sample space is the probability that an event in that subset occurs. Probabilists put measures on all sorts of spaces, including function spaces. As an example, if the sample space is $C([0, 1])$, we could ask ourselves what is the measure of the subset of differentiable functions.

	\item Intergration theory. Measure theory lets us define the Lebesgue integral, which allows us to integrate more functions and gives us more general theorems for when we can exchange limits and integration.
\end{enumerate}

\subsection{Measures}

With this motivation in mind, let us define a measure on an arbitrary set $X$.

\begin{definition}A \emph{measure} on a set $X$ is a function $\mu$ on subsets of $X$ with the following properties:
\begin{enumerate}[(i)]
\item $\mu(E) \in [0, \infty]$ ($\infty$ is allowed!)
\item $\mu(\varnothing) = 0$
\item If the sets $\{E_k\}_{k \in \N}$ are disjoint, then 
	\begin{equation}
	\mu \left( \bigcup_{k=1}^\infty E_k \right) = \sum_{k=1}^\infty \mu(E_k)
	\end{equation}
	This last property is called \emph{countable additivity}
\end{enumerate}
\end{definition}

Other properties we might like $\mu$ to have are the following
\begin{enumerate}[(i)]\setcounter{enumi}{3}
\item $\mu$ is invariant under symmetries such as translations, rotations, and reflfections.
\item $\mu$ agrees with our common notion of length, area, and volume. For example, $\mu([0,1]) = 1$.
\end{enumerate}

Notice that I intentionally left the domain of $\mu$ vague. We would like the domain of $\mu$ to be as large as possible. Ideally, we would like it to be all subsets of $X$. However, as you will see in the beginning of your analysis course, it is impossible to construct a measure on all subsets of $\R$ with these five properties. The good news is that the construction involves a ``nasty'' subset of $\R$, so if we restrict ourselves to a collection of ``nice'' subsets, we should be able to make this work. Our goal is to find a collection of subsets of on which we can define a measure which is as large as possible and contains everything we care about. The collection we will need is called a $\sigma$-algebra.

\begin{definition}A collection $\mathcal{M}$ of subsets of $X$ is a \emph{$\sigma$-algebra} if 
\begin{enumerate}[(i)]
	\item $\varnothing \in \mathcal{M}$ 
	\item $\mathcal{M}$ is closed under complement, i.e. $E \in \mathcal{M} \implies X\setminus E \in \mathcal{M}$
	\item $\mathcal{M}$ is closed under countable unions, i.e. 
	\[
	\{E_k\}_{k \in \N} \subset \mathcal{M} \implies \bigcup_{k=1}^\infty E_k \in \mathcal{M} 
	\]
\end{enumerate}
An \emph{algebra} is the same thing except it is only closed under finite unions.
\end{definition}

If we want to define a measure on a $\sigma-$algebra, the fact that it is closed under countable unions is crucial for the countable additivity property of a measure to make sense. Other properties of $\sigma-$algebras which follow from the definition and basic set theory include
\begin{enumerate}[(i)]\setcounter{enumi}{3}
	\item $X \in \mathcal{M}$ 
	\item $\mathcal{M}$ is closed under countable intersection. This follows from the De Morgan's laws.
	\item $\mathcal{M}$ is closed under relative complement, i.e. $E, F \in \mathcal{M} \implies E \setminus F \in \mathcal{M}$
\end{enumerate}

Here are some easy examples of $\sigma$-algebras on $X$.
\begin{enumerate}
    \item $M = \{ \varnothing, X \}$ (smallest $\sigma$-algebra)
    \item $M = \{ A, A^c, \varnothing, X\}$ ($\sigma$-algebra generated by subset $A$)
    \item $M = \{ \text{all subsets of X} \}$ (largest $\sigma$-algebra, also called the power set of $X$)
\end{enumerate}

The first two are not very interesting. The power set of $X$ is typically too large to define a measure on. An exception is if $X$ is a countable or finite set. Let $X$ be a countable set and let $\mu$ be a measure. As long as we know $\mu(\{x\})$ for all single-point sets $\{x\}$, then for any subset $S$ of $X$,
\[
\mu(S) = \sum_{x \in S} \mu(\{x\})
\]
where the sum is countable or finite.

The first $\sigma$-algebra which is actually of interest is the $\sigma$-algebra generated by a collection of subsets.

\begin{definition}Let $\mathcal{E}$ be a collection of subsets of $X$. Then the \emph{$\sigma$-algebra generated by $\mathcal{E}$}, denoted $\mathcal{M}(\mathcal{E})$, is the unique smallest $\sigma$-algebra containing $\mathcal{E}$. To be precise, we can define $\mathcal{E}$ in one of these two equivalent ways.
\begin{enumerate}[(i)]
\item $\mathcal{M}(\mathcal{E})$ is the intersection of all $\sigma-$algebras containing $\mathcal{E}$.
\item If $\mathcal{N}$ is another $\sigma$-algebra containing $\mathcal{E}$, then $\mathcal{M}(\mathcal{E}) \subset \mathcal{N}$.
\end{enumerate}
\end{definition}

It is worth noting that $\sigma-$algebras are huge and unwieldy enough that there is essentially no way of writing down an arbitrary element of $\mathcal{M}(\mathcal{E})$ in terms of elements of $\mathcal{E}$. The most important of these $\sigma$-algebras is the Borel $\sigma$-algebra.

\begin{definition}
The \emph{Borel $\sigma$-algebra} on $X$, denoted $\mathcal{B}_X$, is the $\sigma$-algebra generated by the collection of open sets of $X$.
\end{definition}

The Borel $\sigma-$algebra contains all open sets, all closed sets, all countable unions and intersections of open and closed sets (some of which may be neither open nor closed), etc. There is no nice way to write down a generic element of $\mathcal{B}_X$. For the special case of the Borel $\sigma-$algebra on $\R$, we have a nice result.

\begin{proposition}The Borel $\sigma-$algebra on $\R$ is generated by 
\begin{enumerate}
	\item All open intervals, i.e. all intervals of the form $(a, b)$
	\item All intervals of one of the following forms: $[a, b], (a, b], [a, b)$.
	\item All rays of one of the following forms $(a, \infty), [a, \infty), (-\infty, b), (-\infty, b]$
\end{enumerate}
\begin{proof}
Let $\mathcal{E}$ be one of these collections. Since $\mathcal{E} \in \mathcal{B}_\R$, $\mathcal{M}(\mathcal{E}) \in \mathcal{B}_\R$. All we have do now is show that $\mathcal{M}(\mathcal{E})$ contains all open sets. For (i), we showed earlier that every open set in $\R$ is the countable union of disjoint open intervals. For (ii) and (iii), all we have to do is show we can construct an open interval $(a, b)$ using countably many set theory operators. For example,
\[
(a, b) = \bigcap_{n=1}^\infty \left[ a - \frac{1}{n}, b + \frac{1}{n}  \right]
\]
\end{proof}
\end{proposition}

Now that we have discussed $\sigma$-algebras, we will return to measures, which, after all, is the whole point of all of this. From now on, let $X$ be a set and $\mu$ a measure defined on a sigma algebra $\mathcal{M}$. Sometimes, this set of three related objects is written $(X, \mu, \mathcal{M})$ or $(X, \mathcal{M}, \mu)$ and is called a measure space. For now, we assume that $\mu$ is defined on all of $\mathcal{M}$. In applications, we will generally construct $\mu$ and $\mathcal{M}$ together so that everyone plays together nicely.

A measure $\mu$ has many nice properties, which we summarize in the following proposition.

\begin{proposition}
Let $X$ be a set and $\mu$ a measure defined on a sigma algebra $\mathcal{M}$. Then $\mu$ has the following properties.
\begin{enumerate}
    \item (Monotonicity) If $E \subset F$, $\mu(E) \leq \mu(F)$. If in addition $\mu(E) < \infty$, then $\mu(F\setminus E) = \mu(F) - \mu(E)$
    \item (Countable Subadditivity) For any sets $E_n$ (not necessarily disjoint), 
    \[
    \mu\left( \bigcup_{n=1}^\infty E_n \right) \leq \sum_{n=1}^\infty \mu(E_n)
    \]
    \item (Continuity from Below) If $E_n$ is an increasing sequence of nested sets, i.e. $E_n \subset E_{n+1}$, then
    \[
    \mu\left( \bigcup_{n=1}^\infty E_n \right) = \lim_{n\rightarrow\infty} \mu(E_n)
    \]
    \item (Continuity from Above) If $E_n$ is an decreasing sequence of nested sets, i.e. $E_n \supset E_{n+1}$ and $\mu(E_1) < \infty$, then
    \[
    \mu\left( \bigcap_{n=1}^\infty E_n \right) = \lim_{n\rightarrow\infty} \mu(E_n)
    \]
\end{enumerate}
\end{proposition}

With that taken care of, it is time to construct our first measure. We will construct the Lebesgue measure, often denoted $m$, which measures length of subsets of $\R$. It gives the ``correct'' measure for intervals, but will in addition allow us to measure many other sets. The contruction we will use is one I find intuitive. You will probably do this a little differently in analysis class. That version has the advantage of letting you construct a whole class of measures for which the Lebesgue measure is a special case; however, it requires more definitions and theorems, and is one more step removed from what is actually going on. We will do our construction in the following steps.

\begin{enumerate}
    \item Construct something which is roughly what we want, but which we can use on all subsets of $\R$. This is called the Lebesgue outer measure, denoted $m^*$. 
    \item Show that $m^*$ gives the correct length for intervals.
    \item Find a $\sigma$-algebra $\mathcal{L}$ on which $m^*$ is actually a measure. This is the Lebesgue $\sigma$-algebra.
    \item Show that this $\sigma$-algebra contains:
    \begin{enumerate}
        \item All subsets $E$ of $X$ with $m^*(E) = 0$, i.e. all null sets.
        \item The Borel $\sigma-$algebra
    \end{enumerate}
\end{enumerate}

First, we define the Lebesgue outer measure. Consider any subset $E$ of $\R$. We can find always find a countable collection of open intervals $\{ (a_n, b_n) \}$ such that 
\[
E \subset \bigcup_{n=1}^\infty (a_n, b_n)
\]
It makes intuitive sense that we should have
\[
\text{length of }E \leq \sum_{n = 1}^\infty (b_n - a_n)
\]

With this in mind, we define the Lebesgue outer measure.

\begin{definition}
For any subset $E$ of $\R$, the Lebesgue outer measure is the function $m^*$ taking values in $[0, \infty]$ ($\infty$ is allowed!) defined by
\[
m^*(E) = \inf\left\{ \sum_{n = 1}^\infty (b_n - a_n) : E \subset \bigcup_{n=1}^\infty (a_n, b_n) \right\}
\]
\end{definition}
Essentially, we are ``approximating $E$ from the outside'' by open intervals, which is why we call this outer measure. The idea is that we cover $E$ with a countable collection of open intervals and add up their lengths; the outer measure is the infimum over all such covers (which may still be infinite!)

Formally, an outer measure is defined as follows.

\begin{definition}An \emph{outer measure} on a set $X$ is a function $\mu^*$ on \emph{all} subsets of $X$ with the following properties:
\begin{enumerate}[(i)]
\item $\mu^*(E) \in [0, \infty]$ ($\infty$ is allowed!)
\item $\mu(\varnothing) = 0$
\item (Monotonicity) If $E \subset F$ then $\mu^*(E) \leq \mu^*(F)$
\item (Countable subadditivity)
	\begin{equation}
	\mu \left( \bigcup_{k=1}^\infty E_k \right) \leq \sum_{k=1}^\infty \mu(E_k)
	\end{equation}
\end{enumerate}
\end{definition}

Going back to the Lebesgue outer measure, we can show the following.
\begin{enumerate}
    \item $m^*$ is an outer measure
    \item $m^*$ of any interval (open, closed, half-open) is equal to its length.
\end{enumerate}

We can also show that the Lebesgue outer measure of any countable set is 0.

\begin{proposition}
Let $E \subset \R$ be a countable set. Then $m^*(E) = 0$.
\begin{proof}
Since $E$ is countable, write it as the sequence $\{x_n\}$. We now use the ``countable epsilon trick''. Let $\epsilon > 0$. Then cover each $x_n$ with the open interval $I_n = (x_n - r_n, x_n + r_n)$, where
\[
r_n = \frac{1}{2} \frac{\epsilon}{2^n}
\]
Thus each $I_n$ has length $2 r^n = \epsilon/2^n$. Then since the outer measure is the infimum of the total length of all such covers,
\[
m^*(E) \leq \sum_{n=1}^{\infty} \frac{\epsilon}{2^n} = \epsilon
\]
Since $\epsilon$ is arbitrary, $m^*(E) = 0$.
\end{proof}
\end{proposition}

In particular, this implies that $m^*(\Q) = 0$. Next, for any outer measure $\mu^*$, we define the concept of $\mu^*$-measurability. A set $A$ is $\mu^*$-measurable if it behaves nicely. What we mean by that is that it splits any ``test set'' $E$ into two nice pieces. The formal definition is as follows, and is due to Carath\'{e}adory.

\begin{definition}
Let $\mu^*$ be an outer measure on $X$. Then a subset $A \subset X$ is \emph{$\mu^*$-measurable} if for every test set $E \subset X$,
\[
\mu^*(E) = \mu^*(E \cap A) + \mu^*(E \cap A^c)
\]
\end{definition}
In other words, $A$ splits any set $E$ into a part which overlaps $A$ and a part which does not. $A$ is ``nice'' if whenever we do this, the outer measure of $E$ is the sum of the outer measure of those two pieces. You will sometimes see the criterion written
\[
\mu^*(E) = \mu^*(E \cap A) + \mu^*(E \setminus A)
\]

To complete the construction of the Lebesgue measure, we will use the Carath\'{e}adory Extension Theorem, which we will not prove here but will be proved (hopefully!) in your analysis class.

\begin{theorem}[Carath\'{e}adory Extension Theorem]
Let $\mu^*$ be an outer measure on $X$, and let $\mathcal{M}$ be the collection of $\mu^*$-measurable sets. Then
\begin{enumerate}
    \item $\mathcal{M}$ is a $\sigma$-algebra.
    \item $\mu^*$ when restricted to $\mathcal{M}$ is a measure, which we designate $\mu$.
    \item $\mathcal{M}$ contains all $\mu^*$-null sets, i.e. all sets $N$ with $\mu^*(N) = 0$.
\end{enumerate}
\begin{proof}
The proof will not be presented here, but it may be useful later to have an outline of the proof. The basic tool you use is the definition of $\mu^*$ measurability, which is used over and over. Here is the order in which you show things.
\begin{enumerate}
    \item Show $\mathcal{M}$ is closed under complement and finite unions, thus is an algebra. (It suffices to show $A, B \in \mathcal{M} \implies A \cup B \in \mathcal{M}$).
    \item Show $\mu^*$ is finitely additive on $\mathcal{M}$. (It suffices to show for $A, B$ disjoint sets in $\mathcal{M}$ that $\mu^*(A \cup B( = \mu^*(A) + \mu^*(B)$).
    \item Show $\mathcal{M}$ is closed under countable disjoint unions This implies it is closed under countable unions, thus is a $\sigma$-algebra. (To see this take any sequence of set $E_n$. Construct $F_n$ from $E_n$ by deleting anything which overlaps with $E_1, \dots, E_{n-1}$. We can get from $E_n$ to $F_n$ or the other way with a finite number of set operations. Then $\cup E_n = \cup F_n$.)
    \item Show $\mu^*$ is countably additive on $\mathcal{M}$, thus is a measure on $\mathcal{M}$.
    \item Show if $\mu^*(N) = 0$ then $N \in \mathcal{M}$.
\end{enumerate}
\end{proof}
\end{theorem}

The last property is particularly important. If a set has measure 0, in general we can ignore it. (You could argue that the entire point of measure theory is figuring out which sets are null sets so you can ignore them.) This property guarantees that if we construct a measure this way, all $\mu^*$-null sets are actually in the $\sigma$-algebra.

We are almost done. Use the Carath\'{e}adory Extension Theorem on the Lebesgue outer measure $m^*$. Let $\mathcal{L}$ be the resulting $\sigma-$algebra of $m^*$-measurable sets, and let $m$ be the measure we obtain by restricting $m^*$ to $\mathcal{L}$. (We use a different letter here since the Lebesgue measure is the most important one and we want to distinguish it from generic $\sigma$-algebras). All that is left is to show that $\mathcal{L}$ contains the entire Borel $\sigma-$algebra. To do this, it is sufficient to show that $\mathcal{L}$ contains any set which generates the $\sigma-$algebra. Since $\mathcal{L}$ is a $\sigma-$algebra, it must then contain the entire Borel $\sigma-$algebra. 
The easiest way to do this is to show that all open intervals are $\mu^*$-measurable (although we will not actually do this here.) We can also show that the Lebesgue measure $m$ is translation-invariant.

\subsection{Measurable Functions and Integration}

One of the advantages of the Lebesgue integral is that we can use it on a large class of functions as long as we have defined a measure. In order to do make that work, we need to define a measurable function between to measure spaces. Essentially, a measurable function is nice enough that we can deal with it.

\begin{definition}
Let $(X,\mathcal{M})$, $(Y,\mathcal{N})$ be sets together with $\sigma-$algebras. Then $f: X \to Y$ is \emph{$(\mathcal{M},\mathcal{N})$-measurable} (or just \emph{measurable}) if for all $F \in \mathcal{N}$, $f^{-1}(F) \in \mathcal{M}$. 
\end{definition}

This is analogous to our definition of continuous functions. If you define a ``measurable'' set to be one in the $\sigma$-algebra, a measurable function is one where pre-images of measurable sets are measurable. Note that although both of these spaces likely have measures associated with their $\sigma$-algebras, the measures themselves to not figure into this definition. Also, as long as the $\sigma-$algebras of the sets involved ``line up'', compositions of measurable functions are measurable.

Next, consider a function $f: (X,\mathcal{M}) \rightarrow \R$. Unless otherwise specified, we always use the Borel $\sigma$-algebra on the codomain $\R$. Thus we say a function $f: (X,\mathcal{M}) \rightarrow \R$ is $\mathcal{M}$-measurable (of just measurable) if it is $(\mathcal{M},\mathcal{B})$-measurable. Finally, consider functions on $\R$. You will see the following terms.
\begin{enumerate}
    \item $f: \R \rightarrow \R$ is Borel measurable if $f$ is $(\mathcal{B}_\R,\mathcal{B}_\R)$-measurable.
    \item $f: \R \rightarrow \R$ is Lebesgue measurable if $f$ is $(\mathcal{L},\mathcal{B}_\R)$-measurable, where $\mathcal{L}$ is the Lebesgue $\sigma$-algebra on $\R$.
\end{enumerate}
Since $\mathcal{B}_\R \subset \mathcal{L}$, every Borel measurable function is Lebesgue measurable, but the converse is not true.

The problem with the definition of measurability is that it is hard to verify since we don't have a good way to characterize what sets belong to a given $\sigma$-algebra. Luckily, it suffices to check the measurability criterion on a set which generates a $\sigma$-algebra

\begin{proposition}
$(X,\mathcal{M})$, $(Y,\mathcal{N})$ be measure spaces, and suppose $\mathcal{N}$ is the $\sigma$-algebra generated by the collection of sets $\mathcal{E}$. Then $f: X \to Y$ is measurable if and only if $f^{-1}(E) \subset \mathcal{M}$ for all $E \in \mathcal{E}$.

\begin{proof}An outline of the proof is as follows.\\
"$\Rightarrow$" This follows from the definition of measurability, since $\mathcal{E} \subset \mathcal{N}$.\\
"$\Leftarrow$" Define the set
\[
\mathcal{H}=\{A \in \mathcal{N}: f^{-1}(A) \in \mathcal{M}\}
\]
We want to show that $\mathcal{H}$ contains $\mathcal{N}$. From our initial assumption, $\mathcal{H}$ contains $\mathcal{E}$. Next, we show that $\mathcal{H}$ is a $\sigma$-algebra using the definition of a $\sigma$-algebra, set-theory operations, and the fact the the inverse image operator $f^{-1}$ commutes with set theory operations. Since $\mathcal{H}$ is a $\sigma$-algebra containing $\mathcal{E}$, it must also contain $\mathcal{N}$, since $\mathcal{N}$ is the $\sigma$-algebra generated by $\mathcal{E}$.
\end{proof}
\end{proposition}

We have the following corollary.

\begin{corollary}
Every continuous function $f: (X,\mathcal{B}_X) \rightarrow (Y,\mathcal{B}_Y)$ is measurable.
\end{corollary}

We can also define measurability in terms of our generators of the Borel $\sigma$-algebra on $\R$. The infinite rays are more useful than the intervals in this case.

\begin{corollary}
A function $(X,\mathcal{M}) \rightarrow \R$ is measurable if and only if one of the following is true.
\begin{enumerate}
    \item $f^{-1}((a,\infty)) \in \mathcal{M}$ for all $a \in \R$.
    \item $f^{-1}([a,\infty)) \in \mathcal{M}$ for all $a \in \R$.
    \item $f^{-1}((-\infty,a)) \in \mathcal{M}$ for all $a \in \R$.
    \item $f^{-1}((-\infty,a]) \in \mathcal{M}$ for all $a \in \R$.
\end{enumerate}
\end{corollary}

I find the fourth one to be the most useful. If we call the set $f^{-1}(a)$ the level set of $a$, the set 
\[
f^{-1}((-\infty,a]) = \{ x : f(x) \leq a \}
\]
is the sublevel set of $a$. Thus for measurability of real-valued functions, we only have to check the sublevel sets.

Finally, the set of measurable functions is nice, in that pretty much however we slice and dice them, we wind up with a measurable function.

\begin{proposition}
Let $f, g$ and $\{ f_n \}$ measurable functions from $(X, \mathcal{M})$ to $\R$. Then the following are measurable.
\begin{enumerate}
\item $f + g$ and $fg$
\item $\max(f, g)$ and $\min(f, g)$
\item $\sup_n f_n(x)$ and $\inf_n f_n(x)$
\item $\limsup_{n\rightarrow\infty} f_n(x)$ and $\liminf_{n\rightarrow\infty} f_n(x)$
\item $\lim_{n\rightarrow\infty} f_n(x)$, provided the limit exists.
\end{enumerate}
\end{proposition}

Recall that the limit superior and limit inferior of a real-valued sequence $\{ x_n \}$ are defined by
\begin{align*}
\limsup_{n\rightarrow\infty} x_n &= \lim_{n \rightarrow \infty} \left( \sup_{k \geq n} x_k \right) \\
\limsup_{n\rightarrow\infty} x_n &= \lim_{n \rightarrow \infty} \left( \inf_{k \geq n} x_k \right) 
\end{align*}
We can think of the limit superior as the limit of the sequence $\{ y_n \} $, given by
\[
y_n = \sup_{k \geq n} x_k
\]
This is a decreasing sequence, since we is taking the supremum over fewer and fewer terms. The limit superior of $\{x_n \}$ is the limit of this sequence, which is the infimum since the sequence is decreasing. Similarly, the limit inferior is the limit of an increasing sequence. As long as we allow the values $\pm \infty$, the $\limsup_{n\rightarrow\infty} x_n$ and $\liminf_{n\rightarrow\infty} x_n$ exist for all real-valued sequences and
\[
\liminf_{n\rightarrow\infty} x_n 
\leq \limsup_{n\rightarrow\infty} x_n
\]
For the actual limit of a sequence, we have
\[
\lim_{n \rightarrow \infty} x_n \text{ exists} \iff
\limsup_{n\rightarrow\infty} x_n = \liminf_{n\rightarrow\infty} x_n
\]

\subsection{Lebesgue Integral for Measure Spaces}

From now on, we will assume all functions are measurable. This is not an unreasonable assumption, because measurable functions are nice, and we like nice things! Probabilists often deal with nonmeasurable functions, but the rest of us can safely use the heuristic ``every reasonable function is measurable''.

We will now define the Lebesgue integral of real-valued functions on an arbitrary measure space $(X, \mathcal{M}, \mu)$. This is done in three steps. These same three steps recur in many contexts.
\begin{enumerate}
    \item Define the integral for simple functions.
    \item Extend to nonnegative functions (take supremum).
    \item Extend to all real-valued functions (split into positive and negative part).
\end{enumerate}

First, we define a simple function.

\begin{definition}
A \emph{simple function} is a function whose range is a finite set. Let $\phi: (X,\mathcal{M}) \rightarrow \R$ be simple. Then we can write $\phi$ in \emph{standard form} as 
\[
\phi(x) = \sum_{k = 1}^n
y_k \chi_{E_k}(x)
\]
where $y_k \in \R$ (the range of the function is this finite set), $E_k \in \mathcal{M}$ are disjoint, and $\chi_{E_k}(x)$ is the \emph{characteristic function}
\[
\chi_{E_k}(x) = \begin{cases}
    1 & x \in E_k \\
    0 & x \notin E_k
\end{cases}
\]
We can also see that $E_k = f^{-1}(\{ y_k \})$.
\end{definition}

We can show that $\chi_{E_k}(x)$ is a measurable function if and only if $E_k \in \mathcal{M}$. Thus the simple function $\phi(x)$ is measurable since it is the sum of measurable functions. 

We can define the Lebesgue integral of a simple function as follows.

\begin{definition}
Let $\phi(x): (X, \mathcal{M}, \mu) \rightarrow \R$ be a simple function written in standard form as 
\[
\phi(x) = \sum_{k = 1}^n
y_k \chi_{E_k}(x)
\]
Then the integral of $\phi(x)$ is defined by
\[
\int_X \phi d\mu = \sum\limits_{k=1}^n y_k \mu(E_k).
\]
where we always take $0 \cdot \infty = 0$. We note that this integral can take a value of $\infty$. We sometimes just write this as $\int \phi$ for shorthand. 
\end{definition}

We can show that this definition is well-defined, i.e. if we have two different representations of the same simple function, the integral is the same. We can also integrate over a set $A \in \mathcal{M}$ instead of the whole space $X$ (this is analogous to integrating over a bounded interval $[a,b]$ instead of all of $\R$.

\begin{definition}
Let $A \in \mathcal{M}$ and $\phi(x): (X, \mathcal{M}, \mu) \rightarrow \R$ simple. Then we define
\[
\int_A \phi d\mu = \int_X \phi \: \chi_A d \mu
= \sum\limits_{k=1}^n y_k \mu(E_k \cap A)
\]
\end{definition}

The integral has the following nice properties. 

\begin{proposition}
Let $\phi$ and $\psi$ be simple functions $(X, \mathcal{M}, \mu) \rightarrow \R$. Then
\begin{enumerate}
\item $\int (\phi + \psi) = \int \phi + \int \psi$
\item If $c \geq 0$, then $\int c \phi = c \int \phi$
\item If $0 \leq \phi \leq \psi$, then $\int \phi \leq \int \psi$
\item Define $\rho$ on $\mathcal{M}$ by 
\[
\rho(E) = \int_E \phi d\mu
\]
Then $\rho$ is a measure on $\mathcal{M}$.
\end{enumerate}
\end{proposition}

The first three are familiar from calculus. The last one is a little strange, but is really useful for proving things (especially the monotone convergence theorem). It also gives us a second way of constructing a measure.

Now that we have define the integral for simple functions, the next step is to extend it to nonnegative functions. First, we show that we can approximate nonnegative real-valued function with simple functions.

\begin{lemma}[Simple Approximation Lemma]
Let $f: (X,\mathcal{M}) \rightarrow \R$ measurable with $f \geq 0$. Then there is an increasing sequence of nonnegative simple function $\{ \phi_n(x) \} $, i.e. $0 \leq \phi_1 \leq \phi_2 \leq \dots \leq f$ such that $\phi_n \rightarrow f$ pointwise. This convergence is uniform if $f$ is bounded.
\begin{proof}
If the range of $f$ is bounded, partition the range into finitely many points and make cake layers; refine the partition mesh at each step. If the range is unbounded, for each $n$, partition $[0, n]$ with a finer mesh at each step.
\end{proof}
\end{lemma}

Thus it makes sense to define the integral of a nonnegative function in the following way.

\begin{definition}
Let $f:(X, \mathcal{M}, \mu) \rightarrow \R$ be a measurable, nonnegative function. Then we define the integral of $f$ by
\[
\int_X fd\mu =\sup\left\{ \int_X \phi d\mu : \phi \text{  simple}, 0\leq\phi\leq f \right\}.
\]
Note that this can be infinite. We say that a nonnegative function $f$ is integrable if this integral is finite.
\end{definition}
The same properties for the integral of simple functions hold for the integral of nonnegative functions.

Finally, we define the integral of real-valued functions. First, define the positive and negative parts of a function $f$ as 
\begin{align*}
f^+ &= \max\{f,0\} \\
f^- &= \max\{-f,0\}
\end{align*}
Note that $f = f^+ - f^-$ and $|f| = f^+ + f^-$. We now have the following definition 

\begin{definition}
Let $f:(X, \mathcal{M}, \mu) \rightarrow \R$ be a measurable function. Then we say $f$ is integrable if $|f|$ is integrable, i.e. $\int_X |f| d\mu < \infty$. In that case, we define
\[
\int_X f d\mu=\int_Xf^{+}d\mu-\int_Xf^-d\mu
\]
\end{definition}

The integral now has all of the familiar properties.

\begin{proposition}
Let $f, g:(X, \mathcal{M}, \mu) \rightarrow \R$ be integrable. Then the integral has the following properties
\begin{enumerate}
\item Linearity: $\int(c f + g) = c \int f + \int g$
\item Comparison: If $f\leq g$, then $\int f \leq \int g$
\item Monotonicity: If $f\geq 0$ and $A\subset B$, then $\int_A f \leq\int_B f$, where $\int_A f = \int f \chi_A$
\item $|\int f| \leq \int |f| $.
\end{enumerate}
\end{proposition}

Before we conclude this section, we note the special role played by sets of measure 0, which are also known as null sets. We say that a particular property holds ``almost everywhere'' (abbreviated ``a.e.'') if it holds everywhere except a set of measure zero. Examples include: a function which is continuous almost everywhere; a function which is 0 almost everywhere; a function which is bounded almost everywhere; two functions which are equal almost everywhere; and a sequence of functions which converges almost everywhere.

We have the following proposition which gives us a way to characterize functions which are 0 almost everywhere.

\begin{proposition}
Let $f: X \rightarrow \R$ be a nonnegative function. Then $f = 0$ almost everywhere if and only if $\int f = 0$.
\begin{proof}
"$\Rightarrow$" We first prove this for a simple function $\phi$. Write $\phi$ in standard form as 
\[
\phi(x) = \sum_{k = 1}^n
y_k \chi_{E_k}(x)
\]
If $\phi = 0$ almost everywhere, either $y_k = 0$ or $\mu(E_k) = 0$ (or both!) for $k = 1, \dots, n$. Thus, using the definition of the integral of a simple function, $\int \phi = 0$. Now take any nonnegative function $f$ with $f = 0$ almost everywhere. For any simple function $\phi$ with $0 \leq \phi \leq f$, $\phi = 0$ almost everywhere as well. Thus by the definition of the integral of nonnegative functions,
\[
\int f = \sup\left\{ \int \phi \text{ simple with }0 \leq \phi \leq f \right\} = 0
\]
"$\Leftarrow$" Let $E = \{ x \in X : f(x) > 0 \}$. Then 
\[
E = \bigcup_{n=1}^\infty E_n
\]
where
\[
E = \left\{ x \in X : f(x) > \frac{1}{n} \right\}
\]
If $f$ is not 0 almost everywhere, then one of the sets $E_n$ has positive measure, i.e. $\mu(E_n) = r > 0$ for some $n$. But then we have
\[
f \geq \frac{1}{n}\chi_{E_n}
\]
and so
\[
\int f \geq \int \frac{1}{n}\chi_{E_n} = \frac{1}{n} \mu(E)n = \frac{r}{n} > 0
\]
\end{proof}
\end{proposition}

Similarly, for a real-valued function $f:X \rightarrow \R$ we have $f = 0$ almost everywhere if and only if $\int |f| = 0$.

Recall that a say a function $f:X \rightarrow \R$ is integrable if $\int |f| < \infty$. From the integration properties, the set of integrable, real-valued functions is closed under addition and scalar multiplication, thus is a vector space. We will call this vector space $L^1$ or $L^1(X)$. We would like to define a natural norm on this vector space by
\[
\|f\| = \int_X |f|
\]
We call this the $L^1$ norm, sometimes written $\|f\|_{L^1}$
Since we are integrating $|f|$, this is nonnegative, and is finite since we restrict ourselves to integrable functions. The triangle inequality follows from the triangle inequality on $\R$ together with the linearity of the integral. Similarly, we can pull constants out by the linearity of the integral. If $f = 0$, then $\int |f| = 0$. However, $\|f\| = 0$ does not imply $f = 0$, only $f = 0$ almost everywhere. Thus we do not have a norm on this vector space of integrable functions.

It seems like we are in a bit of a bind here. However, we can cheat our way out of it. If two functions $f$ and $g$ are equal almost everywhere, they are ``basically the same''; they may differ on a null set, but for most purposes we don't care about that. In our particular case, if $f$ and $g$ are integrable and are equal almost everywhere $\int f = \int g$. Thus, instead of talking about functions, we will talk about equivalence classes of functions, where $f \sim g$ if $f = g$ almost everywhere. In this sense, the $L^1$ norm becomes a true norm on the vector space of these equivalence classes of integrable functions. 

Since we can basically ignore null sets, most of the time we can ignore this equivalence class stuff as well. When we write $f \in L^1$, what we are saying is that $f$ is an integrable function which is defined almost everywhere, which is good enough for most purposes.

\subsection{Convergence Theorems}

Finally, we present the three big convergence theorems, which are perhaps the main advantage of the Lebesgue integration theory. You will prove them in the analysis course.

\begin{theorem}[Monotone Convergence Theorem]
Let $f_n: (X,\mathcal{M},\mu) \rightarrow \R$ measurable with $f_n \geq 0$. If $f_n \uparrow f$ pointwise, then
\[
\lim_{n\rightarrow \infty} \int_X f_n d\mu = \int_X \left(\lim_{n\rightarrow \infty} f_n\right) d\mu= 
\int_X f d\mu 
\]
where this can be infinite.
\end{theorem}

\begin{theorem}[Fatou's Lemma]
Let $f_n: (X,\mathcal{M},\mu) \rightarrow \R$ measurable with $f_n \geq 0$. Then
\begin{align*}
\int \liminf_{n\rightarrow \infty} f_n \leq \liminf_{n\rightarrow \infty} \int f_n
\end{align*}
\end{theorem}

\begin{theorem}[Lebesgue Dominated Convergence Theorem]
Let $f_n: (X,\mathcal{M},\mu) \rightarrow \R$ measurable with $f_n(x) \rightarrow f(x)$ pointwise. If there exists an integrable function $g$ such that $|f_n| \leq g$ for all $n$, then $f$ is integrable and
\begin{align*}
\lim_{n\rightarrow \infty} \int_X f_n d\mu = \int_X \left(\lim_{n\rightarrow \infty} f_n\right) d\mu = \int_X f d\mu.
\end{align*}
\end{theorem}



\end{document}

